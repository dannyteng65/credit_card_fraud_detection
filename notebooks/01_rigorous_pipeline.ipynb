{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "042f6a49",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "Step 0: åŸºæœ¬è¨­å®šèˆ‡è³‡æ–™è®€å–\n",
      "================================================================================\n",
      "\n",
      "âš™ï¸ Optuna èª¿åƒæ¬¡æ•¸è¨­å®š: 20 trials\n",
      "   ğŸ’¡ æç¤ºï¼š\n",
      "      - 10 trials: å¿«é€Ÿæ¸¬è©¦ï¼ˆç´„ 5-10 åˆ†é˜ï¼‰\n",
      "      - 20 trials: å¹³è¡¡é¸æ“‡ï¼ˆç´„ 10-20 åˆ†é˜ï¼‰â­ æ¨è–¦\n",
      "      - 50 trials: æ·±åº¦æœç´¢ï¼ˆç´„ 25-40 åˆ†é˜ï¼‰\n",
      "      - 100 trials: æ¥µè‡´å„ªåŒ–ï¼ˆç´„ 50-80 åˆ†é˜ï¼‰\n",
      "\n",
      "âœ“ æ‰€æœ‰å¥—ä»¶åŒ¯å…¥æˆåŠŸ\n",
      "âœ“ éš¨æ©Ÿç¨®å­è¨­å®šç‚º: 42\n",
      "âœ“ è¼¸å‡ºè·¯å¾‘: outputs\n",
      "\n",
      "æ­£åœ¨è®€å–è³‡æ–™: data/raw/creditcard.csv\n"
     ]
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'data/raw/creditcard.csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 89\u001b[0m\n\u001b[1;32m     86\u001b[0m DATA_PATH \u001b[38;5;241m=\u001b[39m PROJECT_ROOT \u001b[38;5;241m/\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdata\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m/\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mraw\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m/\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcreditcard.csv\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     88\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mæ­£åœ¨è®€å–è³‡æ–™: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mDATA_PATH\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m---> 89\u001b[0m df \u001b[38;5;241m=\u001b[39m \u001b[43mpd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread_csv\u001b[49m\u001b[43m(\u001b[49m\u001b[43mDATA_PATH\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     90\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mâœ“ è³‡æ–™è®€å–å®Œæˆ\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     92\u001b[0m \u001b[38;5;66;03m# 0-4. é¡¯ç¤ºåŸºæœ¬è³‡è¨Š\u001b[39;00m\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/pandas/io/parsers/readers.py:1026\u001b[0m, in \u001b[0;36mread_csv\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options, dtype_backend)\u001b[0m\n\u001b[1;32m   1013\u001b[0m kwds_defaults \u001b[38;5;241m=\u001b[39m _refine_defaults_read(\n\u001b[1;32m   1014\u001b[0m     dialect,\n\u001b[1;32m   1015\u001b[0m     delimiter,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1022\u001b[0m     dtype_backend\u001b[38;5;241m=\u001b[39mdtype_backend,\n\u001b[1;32m   1023\u001b[0m )\n\u001b[1;32m   1024\u001b[0m kwds\u001b[38;5;241m.\u001b[39mupdate(kwds_defaults)\n\u001b[0;32m-> 1026\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_read\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/pandas/io/parsers/readers.py:620\u001b[0m, in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    617\u001b[0m _validate_names(kwds\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnames\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m))\n\u001b[1;32m    619\u001b[0m \u001b[38;5;66;03m# Create the parser.\u001b[39;00m\n\u001b[0;32m--> 620\u001b[0m parser \u001b[38;5;241m=\u001b[39m \u001b[43mTextFileReader\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    622\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m chunksize \u001b[38;5;129;01mor\u001b[39;00m iterator:\n\u001b[1;32m    623\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m parser\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/pandas/io/parsers/readers.py:1620\u001b[0m, in \u001b[0;36mTextFileReader.__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m   1617\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhas_index_names\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m kwds[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhas_index_names\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[1;32m   1619\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles: IOHandles \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m-> 1620\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_engine \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_make_engine\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mengine\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/pandas/io/parsers/readers.py:1880\u001b[0m, in \u001b[0;36mTextFileReader._make_engine\u001b[0;34m(self, f, engine)\u001b[0m\n\u001b[1;32m   1878\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m mode:\n\u001b[1;32m   1879\u001b[0m         mode \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m-> 1880\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles \u001b[38;5;241m=\u001b[39m \u001b[43mget_handle\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1881\u001b[0m \u001b[43m    \u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1882\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1883\u001b[0m \u001b[43m    \u001b[49m\u001b[43mencoding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mencoding\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1884\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcompression\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcompression\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1885\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmemory_map\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmemory_map\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1886\u001b[0m \u001b[43m    \u001b[49m\u001b[43mis_text\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mis_text\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1887\u001b[0m \u001b[43m    \u001b[49m\u001b[43merrors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mencoding_errors\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstrict\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1888\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstorage_options\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1889\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1890\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1891\u001b[0m f \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles\u001b[38;5;241m.\u001b[39mhandle\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/pandas/io/common.py:873\u001b[0m, in \u001b[0;36mget_handle\u001b[0;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[1;32m    868\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(handle, \u001b[38;5;28mstr\u001b[39m):\n\u001b[1;32m    869\u001b[0m     \u001b[38;5;66;03m# Check whether the filename is to be opened in binary mode.\u001b[39;00m\n\u001b[1;32m    870\u001b[0m     \u001b[38;5;66;03m# Binary mode does not support 'encoding' and 'newline'.\u001b[39;00m\n\u001b[1;32m    871\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m ioargs\u001b[38;5;241m.\u001b[39mencoding \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m ioargs\u001b[38;5;241m.\u001b[39mmode:\n\u001b[1;32m    872\u001b[0m         \u001b[38;5;66;03m# Encoding\u001b[39;00m\n\u001b[0;32m--> 873\u001b[0m         handle \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\n\u001b[1;32m    874\u001b[0m \u001b[43m            \u001b[49m\u001b[43mhandle\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    875\u001b[0m \u001b[43m            \u001b[49m\u001b[43mioargs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    876\u001b[0m \u001b[43m            \u001b[49m\u001b[43mencoding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mioargs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mencoding\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    877\u001b[0m \u001b[43m            \u001b[49m\u001b[43merrors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43merrors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    878\u001b[0m \u001b[43m            \u001b[49m\u001b[43mnewline\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    879\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    880\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    881\u001b[0m         \u001b[38;5;66;03m# Binary mode\u001b[39;00m\n\u001b[1;32m    882\u001b[0m         handle \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mopen\u001b[39m(handle, ioargs\u001b[38;5;241m.\u001b[39mmode)\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'data/raw/creditcard.csv'"
     ]
    }
   ],
   "source": [
    "# =========================\n",
    "# Step 0: åŸºæœ¬è¨­å®šèˆ‡è³‡æ–™è®€å–\n",
    "# =========================\n",
    "\n",
    "print(\"=\"*80)\n",
    "print(\"Step 0: åŸºæœ¬è¨­å®šèˆ‡è³‡æ–™è®€å–\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# 0-1. åŒ¯å…¥æ‰€æœ‰éœ€è¦çš„å¥—ä»¶\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import warnings\n",
    "from datetime import datetime\n",
    "from pathlib import Path\n",
    "import os\n",
    "\n",
    "# æ©Ÿå™¨å­¸ç¿’ç›¸é—œ\n",
    "from sklearn.model_selection import train_test_split, StratifiedKFold, cross_val_score\n",
    "from sklearn.preprocessing import RobustScaler\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import (\n",
    "    classification_report, confusion_matrix,\n",
    "    f1_score, precision_score, recall_score,\n",
    "    roc_auc_score, precision_recall_curve,\n",
    "    ConfusionMatrixDisplay\n",
    ")\n",
    "\n",
    "# è™•ç†ä¸å¹³è¡¡è³‡æ–™\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "from imblearn.pipeline import Pipeline as ImbPipeline\n",
    "\n",
    "# æ¨¡å‹\n",
    "import xgboost as xgb\n",
    "import lightgbm as lgb\n",
    "\n",
    "# è¶…åƒæ•¸èª¿æ•´\n",
    "import optuna\n",
    "\n",
    "# æ¨¡å‹è§£é‡‹\n",
    "import shap\n",
    "from lime import lime_tabular\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "plt.style.use('seaborn-v0_8-darkgrid')\n",
    "\n",
    "# 0-2. è¨­å®šéš¨æ©Ÿç¨®å­ï¼Œç¢ºä¿çµæœå¯é‡ç¾\n",
    "RANDOM_STATE = 42\n",
    "np.random.seed(RANDOM_STATE)\n",
    "\n",
    "# 0-2.1 è¨­å®š Optuna èª¿åƒæ¬¡æ•¸ï¼ˆå¯è‡ªè¡Œèª¿æ•´ï¼‰â­\n",
    "OPTUNA_TRIALS = 20  # ğŸ”§ èª¿æ•´é€™å€‹æ•¸å­—ä¾†æ§åˆ¶ Optuna çš„æœç´¢æ¬¡æ•¸\n",
    "print(f\"\\nâš™ï¸ Optuna èª¿åƒæ¬¡æ•¸è¨­å®š: {OPTUNA_TRIALS} trials\")\n",
    "print(\"   ğŸ’¡ æç¤ºï¼š\")\n",
    "print(\"      - 10 trials: å¿«é€Ÿæ¸¬è©¦ï¼ˆç´„ 5-10 åˆ†é˜ï¼‰\")\n",
    "print(\"      - 20 trials: å¹³è¡¡é¸æ“‡ï¼ˆç´„ 10-20 åˆ†é˜ï¼‰â­ æ¨è–¦\")\n",
    "print(\"      - 50 trials: æ·±åº¦æœç´¢ï¼ˆç´„ 25-40 åˆ†é˜ï¼‰\")\n",
    "print(\"      - 100 trials: æ¥µè‡´å„ªåŒ–ï¼ˆç´„ 50-80 åˆ†é˜ï¼‰\")\n",
    "\n",
    "# 0-3. è¨­å®šè¼¸å‡ºè·¯å¾‘\n",
    "OUTPUT_DIR = '/Users/'\n",
    "import os\n",
    "os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
    "\n",
    "print(\"\\nâœ“ æ‰€æœ‰å¥—ä»¶åŒ¯å…¥æˆåŠŸ\")\n",
    "print(f\"âœ“ éš¨æ©Ÿç¨®å­è¨­å®šç‚º: {RANDOM_STATE}\")\n",
    "print(f\"âœ“ è¼¸å‡ºè·¯å¾‘: {OUTPUT_DIR}\")\n",
    "\n",
    "# 0-3. è®€å–è³‡æ–™\n",
    "DATA_PATH = '/Users/creditcard.csv'\n",
    "\n",
    "print(f\"\\næ­£åœ¨è®€å–è³‡æ–™: {DATA_PATH}\")\n",
    "df = pd.read_csv(DATA_PATH)\n",
    "print(\"âœ“ è³‡æ–™è®€å–å®Œæˆ\")\n",
    "\n",
    "# 0-4. é¡¯ç¤ºåŸºæœ¬è³‡è¨Š\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"è³‡æ–™åŸºæœ¬è³‡è¨Š\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# è³‡æ–™åˆ—æ•¸èˆ‡æ¬„ä½æ•¸\n",
    "print(f\"\\nè³‡æ–™å½¢ç‹€: {df.shape}\")\n",
    "print(f\"  - ç¸½äº¤æ˜“ç­†æ•¸: {df.shape[0]:,}\")\n",
    "print(f\"  - ç¸½æ¬„ä½æ•¸: {df.shape[1]}\")\n",
    "\n",
    "# å‰ 5 ç­†è³‡æ–™\n",
    "print(\"\\nå‰ 5 ç­†è³‡æ–™:\")\n",
    "print(df.head())\n",
    "\n",
    "# å„æ¬„ä½çš„ç¼ºå¤±å€¼æƒ…æ³\n",
    "print(\"\\nç¼ºå¤±å€¼æª¢æŸ¥:\")\n",
    "missing = df.isnull().sum()\n",
    "if missing.sum() == 0:\n",
    "    print(\"âœ“ æ²’æœ‰ç¼ºå¤±å€¼\")\n",
    "else:\n",
    "    print(missing[missing > 0])\n",
    "\n",
    "# Class çš„é¡åˆ¥æ¯”ä¾‹\n",
    "print(\"\\nClass é¡åˆ¥åˆ†å¸ƒ:\")\n",
    "class_counts = df['Class'].value_counts()\n",
    "class_ratio = df['Class'].value_counts(normalize=True)\n",
    "\n",
    "print(f\"  æ­£å¸¸äº¤æ˜“ (Class=0): {class_counts[0]:,} ({class_ratio[0]*100:.3f}%)\")\n",
    "print(f\"  è©æ¬ºäº¤æ˜“ (Class=1): {class_counts[1]:,} ({class_ratio[1]*100:.3f}%)\")\n",
    "print(f\"  ä¸å¹³è¡¡æ¯”ä¾‹: {class_counts[0]/class_counts[1]:.1f}:1\")\n",
    "\n",
    "# =========================\n",
    "# Step 1: åˆæ­¥ EDA\n",
    "# =========================\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"Step 1: åˆæ­¥æ¢ç´¢æ€§è³‡æ–™åˆ†æ (EDA)\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# 1-1. Time çš„è¦–è¦ºåŒ–\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Time ç›´æ–¹åœ–\n",
    "axes[0].hist(df['Time'], bins=50, edgecolor='black', alpha=0.7)\n",
    "axes[0].set_title('Time Distribution (Histogram)', fontsize=14, fontweight='bold')\n",
    "axes[0].set_xlabel('Time (seconds)')\n",
    "axes[0].set_ylabel('Frequency')\n",
    "axes[0].grid(alpha=0.3)\n",
    "\n",
    "# Time boxplot\n",
    "axes[1].boxplot(df['Time'], vert=True)\n",
    "axes[1].set_title('Time Distribution (Boxplot)', fontsize=14, fontweight='bold')\n",
    "axes[1].set_ylabel('Time (seconds)')\n",
    "axes[1].grid(alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(os.path.join(OUTPUT_DIR, '01_time_distribution.png'), dpi=300, bbox_inches='tight')\n",
    "plt.close()\n",
    "\n",
    "print(f\"\\nâœ“ Time è¦–è¦ºåŒ–å®Œæˆ (å·²å­˜æª”: {OUTPUT_DIR}/01_time_distribution.png)\")\n",
    "\n",
    "# 1-2. Amount çš„è¦–è¦ºåŒ–\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Amount ç›´æ–¹åœ–\n",
    "axes[0].hist(df['Amount'], bins=50, edgecolor='black', alpha=0.7)\n",
    "axes[0].set_title('Amount Distribution (Histogram)', fontsize=14, fontweight='bold')\n",
    "axes[0].set_xlabel('Amount')\n",
    "axes[0].set_ylabel('Frequency')\n",
    "axes[0].grid(alpha=0.3)\n",
    "\n",
    "# Amount boxplot\n",
    "axes[1].boxplot(df['Amount'], vert=True)\n",
    "axes[1].set_title('Amount Distribution (Boxplot)', fontsize=14, fontweight='bold')\n",
    "axes[1].set_ylabel('Amount')\n",
    "axes[1].grid(alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(os.path.join(OUTPUT_DIR, '02_amount_distribution.png'), dpi=300, bbox_inches='tight')\n",
    "plt.close()\n",
    "\n",
    "print(f\"âœ“ Amount è¦–è¦ºåŒ–å®Œæˆ (å·²å­˜æª”: {OUTPUT_DIR}/02_amount_distribution.png)\")\n",
    "\n",
    "# 1-3. è¼¸å‡ºè§€å¯Ÿçµè«–\n",
    "print(\"\\n\" + \"-\"*80)\n",
    "print(\"EDA è§€å¯Ÿçµè«–:\")\n",
    "print(\"-\"*80)\n",
    "\n",
    "print(\"\\né—œæ–¼ Time:\")\n",
    "print(f\"  - ç¯„åœ: {df['Time'].min():.0f} ~ {df['Time'].max():.0f} ç§’\")\n",
    "print(f\"  - å¹³å‡: {df['Time'].mean():.0f} ç§’\")\n",
    "print(f\"  - ä¸­ä½æ•¸: {df['Time'].median():.0f} ç§’\")\n",
    "print(\"  - è§€å¯Ÿ: Time é¡¯ç¤ºå…©å€‹æ˜é¡¯çš„äº¤æ˜“é«˜å³°æœŸï¼Œå¯èƒ½ä»£è¡¨ä¸åŒå¤©æˆ–æ™‚æ®µ\")\n",
    "\n",
    "print(\"\\nAmount:\")\n",
    "print(f\"  - ç¯„åœ: ${df['Amount'].min():.2f} ~ ${df['Amount'].max():.2f}\")\n",
    "print(f\"  - å¹³å‡: ${df['Amount'].mean():.2f}\")\n",
    "print(f\"  - ä¸­ä½æ•¸: ${df['Amount'].median():.2f}\")\n",
    "print(f\"  - 95% åˆ†ä½æ•¸: ${df['Amount'].quantile(0.95):.2f}\")\n",
    "print(f\"  - 99% åˆ†ä½æ•¸: ${df['Amount'].quantile(0.99):.2f}\")\n",
    "print(\"  - è§€å¯Ÿ: Amount éå¸¸åæ…‹ï¼Œæœ‰é•·å°¾åˆ†å¸ƒã€‚å¤§éƒ¨åˆ†äº¤æ˜“é‡‘é¡å¾ˆå°ï¼Œå°‘æ•¸äº¤æ˜“é‡‘é¡å¾ˆå¤§\")\n",
    "print(\"  - å»ºè­°: æ‡‰è©²å° Amount åšå°æ•¸è½‰æ› (log transformation)\")\n",
    "\n",
    "# =========================\n",
    "# Step 2: ç‰¹å¾µå·¥ç¨‹ (ä¸åˆªæ¬„ä½ï¼Œåªæ–°å¢)\n",
    "# =========================\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"Step 2: ç‰¹å¾µå·¥ç¨‹ (Feature Engineering)\")\n",
    "print(\"=\"*80)\n",
    "print(\"\\né‡è¦: æˆ‘å€‘åªæ–°å¢ç‰¹å¾µï¼Œä¸åˆªé™¤ä»»ä½•åŸå§‹æ¬„ä½\")\n",
    "\n",
    "# è¤‡è£½ä¸€ä»½è³‡æ–™ä»¥é¿å…ä¿®æ”¹åŸå§‹è³‡æ–™\n",
    "df_fe = df.copy()\n",
    "\n",
    "# 2-1. Time ç›¸é—œç‰¹å¾µ\n",
    "print(\"\\nè™•ç† Time ç‰¹å¾µ...\")\n",
    "\n",
    "# time_diff: ç•¶å‰äº¤æ˜“èˆ‡å‰ä¸€ç­†äº¤æ˜“çš„æ™‚é–“å·®\n",
    "df_fe['time_diff'] = df_fe['Time'].diff().fillna(0)\n",
    "print(\"  âœ“ æ–°å¢ time_diff (èˆ‡å‰ä¸€ç­†äº¤æ˜“çš„æ™‚é–“å·®)\")\n",
    "\n",
    "# time_sin, time_cos: å°‡ Time ç•¶ä½œé€±æœŸæ€§æ•¸å€¼\n",
    "df_fe['time_norm'] = df_fe['Time'] / df_fe['Time'].max()\n",
    "df_fe['time_sin'] = np.sin(2 * np.pi * df_fe['time_norm'])\n",
    "df_fe['time_cos'] = np.cos(2 * np.pi * df_fe['time_norm'])\n",
    "print(\"  âœ“ æ–°å¢ time_sin, time_cos (é€±æœŸæ€§ç‰¹å¾µ)\")\n",
    "\n",
    "# 2-2. Amount ç›¸é—œç‰¹å¾µ\n",
    "print(\"\\nè™•ç† Amount ç‰¹å¾µ...\")\n",
    "\n",
    "# log_amount: log1p è½‰æ›ï¼ˆè™•ç†åæ…‹åˆ†å¸ƒï¼‰\n",
    "df_fe['log_amount'] = np.log1p(df_fe['Amount'])\n",
    "print(\"  âœ“ æ–°å¢ log_amount (å°æ•¸è½‰æ›)\")\n",
    "\n",
    "# ğŸ’¡ V1~V28 æ˜¯ PCA å¾Œçš„ç‰¹å¾µï¼Œå·²ç¶“æ¨™æº–åŒ–ï¼Œä¸éœ€è¦é¡å¤– scaling\n",
    "print(\"\\nğŸ’¡ V1~V28 æ˜¯ PCA å¾Œçš„ç‰¹å¾µï¼Œå·²ç¶“æ¨™æº–åŒ–ï¼Œä¸éœ€è¦é¡å¤– scaling\")\n",
    "\n",
    "# =========================\n",
    "# Step 2.5: ç­–ç•¥æ€§ç‰¹å¾µå·¥ç¨‹ï¼ˆåŸºæ–¼ SHAP Top ç‰¹å¾µï¼‰â­\n",
    "# =========================\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"Step 2.5: ç­–ç•¥æ€§ç‰¹å¾µå·¥ç¨‹ï¼ˆåŸºæ–¼ SHAP åˆ†æï¼‰\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "print(\"\\nğŸ“Š æ ¹æ“š SHAP åˆ†æï¼Œæœ€é‡è¦çš„ç‰¹å¾µç‚ºï¼š\")\n",
    "print(\"  1. V14 (é‡è¦æ€§ ~5.0) â­â­â­\")\n",
    "print(\"  2. V4  (é‡è¦æ€§ ~2.0) â­â­\")\n",
    "print(\"  3. V12 (é‡è¦æ€§ ~1.2) â­\")\n",
    "print(\"  4. V10 (é‡è¦æ€§ ~1.1) â­\")\n",
    "print(\"  5-10. V11, V3, V17, log_amount, V9, V1\")\n",
    "\n",
    "print(\"\\nğŸ’¡ ç­–ç•¥ï¼šåªé‡å° TOP ç‰¹å¾µåšç²¾æº–åŠ å·¥ï¼Œè€Œéäº‚åŠ  50 å€‹æ¬„ä½\")\n",
    "\n",
    "# (1) log_amount åˆ†ç®± - æ¨¹æ¨¡å‹å–œæ­¡é›¢æ•£ç‰¹å¾µ\n",
    "print(\"\\n 1ï¸âƒ£ log_amount åˆ†ç®±ï¼ˆé›¢æ•£åŒ–ï¼‰\")\n",
    "df_fe['amount_bin'] = pd.qcut(df_fe['log_amount'], q=4, labels=False, duplicates='drop')\n",
    "print(\"  âœ“ æ–°å¢ amount_bin (åˆ†æˆ 4 å€‹ç­‰ç´š)\")\n",
    "print(\"    0=å¾ˆä¾¿å®œ, 1=ä¸­ç­‰, 2=åé«˜, 3=éå¸¸é«˜\")\n",
    "\n",
    "# (2) äº¤äº’ç‰¹å¾µ - æ•æ‰é‡è¦ç‰¹å¾µèˆ‡é‡‘é¡çš„çµ„åˆæ•ˆæ‡‰\n",
    "print(\"\\n 2ï¸âƒ£ äº¤äº’ç‰¹å¾µï¼ˆé‡è¦ç‰¹å¾µ Ã— log_amountï¼‰\")\n",
    "\n",
    "# V14 Ã— log_amountï¼šV14 æ˜¯çµ•å°ç‹è€…ï¼Œèˆ‡é‡‘é¡çš„äº¤äº’å¯èƒ½æ­ç¤ºè©æ¬ºæ¨¡å¼\n",
    "df_fe['V14_x_logA'] = df_fe['V14'] * df_fe['log_amount']\n",
    "print(\"  âœ“ æ–°å¢ V14_x_logA (V14 Ã— log_amount)\")\n",
    "print(\"    â†’ V14 å¥‡æ€ª pattern Ã— å¤§é‡‘é¡ = æ›´å¯ç–‘\")\n",
    "\n",
    "# V4 Ã— log_amountï¼šV4 æ’åç¬¬äºŒ\n",
    "df_fe['V4_x_logA'] = df_fe['V4'] * df_fe['log_amount']\n",
    "print(\"  âœ“ æ–°å¢ V4_x_logA (V4 Ã— log_amount)\")\n",
    "\n",
    "# V12 Ã— log_amountï¼šV12 æ’åç¬¬ä¸‰\n",
    "df_fe['V12_x_logA'] = df_fe['V12'] * df_fe['log_amount']\n",
    "print(\"  âœ“ æ–°å¢ V12_x_logA (V12 Ã— log_amount)\")\n",
    "\n",
    "# (3) TOP å…©å€‹ç‰¹å¾µçš„äº¤äº’\n",
    "print(\"\\n 3ï¸âƒ£ é ‚ç´šç‰¹å¾µäº¤äº’ï¼ˆV14 Ã— V4ï¼‰\")\n",
    "df_fe['V14_x_V4'] = df_fe['V14'] * df_fe['V4']\n",
    "print(\"  âœ“ æ–°å¢ V14_x_V4 (å…©å€‹æœ€é‡è¦ç‰¹å¾µçš„çµ„åˆ)\")\n",
    "print(\"    â†’ æ•æ‰å…©å€‹ä¸»è¦ pattern çš„è¯åˆæ•ˆæ‡‰\")\n",
    "\n",
    "# (4) æ¥µç«¯å€¼æ¨™è¨˜ - æ•æ‰ç•°å¸¸æ¨¡å¼\n",
    "print(\"\\n 4ï¸âƒ£ æ¥µç«¯å€¼æ¨™è¨˜ï¼ˆç•°å¸¸åµæ¸¬ï¼‰\")\n",
    "\n",
    "# V14 çš„æ¥µç«¯å€¼ï¼ˆæ ¹æ“š SHAP åœ–ï¼ŒV14 çš„ç¯„åœå¾ˆå¤§ï¼Œå¾ -8 åˆ° +2ï¼‰\n",
    "v14_q01 = df_fe['V14'].quantile(0.01)\n",
    "v14_q99 = df_fe['V14'].quantile(0.99)\n",
    "df_fe['V14_extreme'] = ((df_fe['V14'] < v14_q01) | (df_fe['V14'] > v14_q99)).astype(int)\n",
    "print(f\"  âœ“ æ–°å¢ V14_extreme (V14 < {v14_q01:.2f} æˆ– > {v14_q99:.2f})\")\n",
    "\n",
    "# V4 çš„æ¥µç«¯å€¼\n",
    "v4_q01 = df_fe['V4'].quantile(0.01)\n",
    "v4_q99 = df_fe['V4'].quantile(0.99)\n",
    "df_fe['V4_extreme'] = ((df_fe['V4'] < v4_q01) | (df_fe['V4'] > v4_q99)).astype(int)\n",
    "print(f\"  âœ“ æ–°å¢ V4_extreme (V4 < {v4_q01:.2f} æˆ– > {v4_q99:.2f})\")\n",
    "\n",
    "# (5) é«˜é‡‘é¡äº¤æ˜“æ¨™è¨˜\n",
    "print(\"\\n 5ï¸âƒ£ é«˜é‡‘é¡äº¤æ˜“æ¨™è¨˜\")\n",
    "amount_q95 = df_fe['log_amount'].quantile(0.95)\n",
    "df_fe['high_amount'] = (df_fe['log_amount'] > amount_q95).astype(int)\n",
    "print(f\"  âœ“ æ–°å¢ high_amount (log_amount > {amount_q95:.2f}ï¼Œå³ top 5%)\")\n",
    "\n",
    "print(\"\\n\" + \"-\"*80)\n",
    "print(\"ç­–ç•¥æ€§ç‰¹å¾µå·¥ç¨‹ç¸½çµ:\")\n",
    "print(\"-\"*80)\n",
    "print(\"æ–°å¢ç‰¹å¾µæ•¸é‡: 8 å€‹ï¼ˆç²¾ç°¡ä½†æœ‰æ•ˆï¼‰\")\n",
    "print(\"\\né¡å‹åˆ†å¸ƒ:\")\n",
    "print(\"  - é›¢æ•£ç‰¹å¾µ: 1 å€‹ (amount_bin)\")\n",
    "print(\"  - äº¤äº’ç‰¹å¾µ: 4 å€‹ (V14_x_logA, V4_x_logA, V12_x_logA, V14_x_V4)\")\n",
    "print(\"  - æ¥µç«¯å€¼æ¨™è¨˜: 3 å€‹ (V14_extreme, V4_extreme, high_amount)\")\n",
    "print(\"\\nâœ… å„ªé»ï¼š\")\n",
    "print(\"  1. åªé‡å° SHAP Top ç‰¹å¾µï¼Œé¿å…éåº¦è¤‡é›œ\")\n",
    "print(\"  2. åŒ…å«é›¢æ•£ã€äº¤äº’ã€æ¥µç«¯å€¼ä¸‰ç¨®é¡å‹\")\n",
    "print(\"  3. æ¨¹æ¨¡å‹å‹å–„ï¼ˆé›¢æ•£å€¼ã€æ¨™è¨˜ï¼‰\")\n",
    "print(\"  4. æ•æ‰æ¥µç«¯ / å¥‡æ€ªçš„çµ„åˆæ¨¡å¼\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# æª¢æŸ¥æ–°å¢çš„ç‰¹å¾µ\n",
    "print(\"\\n\" + \"-\"*80)\n",
    "print(\"ç‰¹å¾µå·¥ç¨‹ç¸½çµ:\")\n",
    "print(\"-\"*80)\n",
    "print(f\"åŸå§‹æ¬„ä½æ•¸: {df.shape[1]}\")\n",
    "print(f\"æ–°å¢å¾Œæ¬„ä½æ•¸: {df_fe.shape[1]}\")\n",
    "print(f\"æ–°å¢ç‰¹å¾µæ•¸: {df_fe.shape[1] - df.shape[1]}\")\n",
    "\n",
    "print(\"\\næ–°å¢çš„ç‰¹å¾µ:\")\n",
    "new_features = [col for col in df_fe.columns if col not in df.columns]\n",
    "\n",
    "# åˆ†é¡é¡¯ç¤º\n",
    "basic_features = [f for f in new_features if f in ['time_diff', 'time_norm', 'time_sin', 'time_cos', 'log_amount']]\n",
    "strategic_features = [f for f in new_features if f not in basic_features]\n",
    "\n",
    "print(\"\\nåŸºç¤ç‰¹å¾µï¼ˆ5 å€‹ï¼‰:\")\n",
    "for i, feat in enumerate(basic_features, 1):\n",
    "    print(f\"  {i}. {feat}\")\n",
    "\n",
    "print(\"\\nç­–ç•¥æ€§ç‰¹å¾µï¼ˆ8 å€‹ï¼ŒåŸºæ–¼ SHAPï¼‰â­:\")\n",
    "for i, feat in enumerate(strategic_features, 1):\n",
    "    print(f\"  {i}. {feat}\")\n",
    "\n",
    "print(\"\\nğŸ’¡ ç­–ç•¥æ€§ç‰¹å¾µèªªæ˜:\")\n",
    "print(\"  - amount_bin: log_amount çš„ 4 åˆ†ä½é›¢æ•£åŒ–\")\n",
    "print(\"  - V14_x_logA, V4_x_logA, V12_x_logA: é‡è¦ç‰¹å¾µèˆ‡é‡‘é¡çš„äº¤äº’\")\n",
    "print(\"  - V14_x_V4: å…©å€‹æœ€é‡è¦ç‰¹å¾µçš„çµ„åˆ\")\n",
    "print(\"  - V14_extreme, V4_extreme: æ¥µç«¯å€¼æ¨™è¨˜\")\n",
    "print(\"  - high_amount: é«˜é‡‘é¡äº¤æ˜“æ¨™è¨˜ï¼ˆtop 5%ï¼‰\")\n",
    "\n",
    "# =========================\n",
    "# Step 3: åˆ‡åˆ†è¨“ç·´é›†ã€é©—è­‰é›†èˆ‡æ¸¬è©¦é›† (åš´è¬¹ç‰ˆæœ¬)\n",
    "# =========================\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"Step 3: åˆ‡åˆ†è¨“ç·´é›†ã€é©—è­‰é›†èˆ‡æ¸¬è©¦é›†\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# 3-1. å®šç¾©ç‰¹å¾µ X å’Œç›®æ¨™è®Šæ•¸ y\n",
    "X = df_fe.drop('Class', axis=1)\n",
    "y = df_fe['Class']\n",
    "\n",
    "print(f\"\\nç‰¹å¾µçŸ©é™£ X: {X.shape}\")\n",
    "print(f\"ç›®æ¨™è®Šæ•¸ y: {y.shape}\")\n",
    "\n",
    "# 3-2. å…ˆåˆ‡åˆ†å‡ºæ¸¬è©¦é›† (20%)\n",
    "X_temp, X_test, y_temp, y_test = train_test_split(\n",
    "    X, y,\n",
    "    test_size=0.2,\n",
    "    stratify=y,\n",
    "    random_state=RANDOM_STATE\n",
    ")\n",
    "\n",
    "# 3-3. å†å¾å‰©é¤˜çš„ 80% ä¸­åˆ‡åˆ†å‡ºé©—è­‰é›† (å¯¦éš›ä½”ç¸½è³‡æ–™çš„ 20%)\n",
    "X_train, X_valid, y_train, y_valid = train_test_split(\n",
    "    X_temp, y_temp,\n",
    "    test_size=0.25,  # 0.25 * 0.8 = 0.2 of total\n",
    "    stratify=y_temp,\n",
    "    random_state=RANDOM_STATE\n",
    ")\n",
    "\n",
    "print(f\"\\nè³‡æ–™åˆ‡åˆ†å®Œæˆ:\")\n",
    "print(f\"  è¨“ç·´é›†: {X_train.shape[0]:,} ç­† ({X_train.shape[0]/len(X)*100:.1f}%)\")\n",
    "print(f\"  é©—è­‰é›†: {X_valid.shape[0]:,} ç­† ({X_valid.shape[0]/len(X)*100:.1f}%)\")\n",
    "print(f\"  æ¸¬è©¦é›†: {X_test.shape[0]:,} ç­† ({X_test.shape[0]/len(X)*100:.1f}%)\")\n",
    "\n",
    "# æª¢æŸ¥é¡åˆ¥æ¯”ä¾‹\n",
    "print(f\"\\nè¨“ç·´é›†é¡åˆ¥åˆ†å¸ƒ:\")\n",
    "train_class_counts = y_train.value_counts()\n",
    "print(f\"  æ­£å¸¸äº¤æ˜“: {train_class_counts[0]:,} ({train_class_counts[0]/len(y_train)*100:.3f}%)\")\n",
    "print(f\"  è©æ¬ºäº¤æ˜“: {train_class_counts[1]:,} ({train_class_counts[1]/len(y_train)*100:.3f}%)\")\n",
    "\n",
    "print(f\"\\né©—è­‰é›†é¡åˆ¥åˆ†å¸ƒ:\")\n",
    "valid_class_counts = y_valid.value_counts()\n",
    "print(f\"  æ­£å¸¸äº¤æ˜“: {valid_class_counts[0]:,} ({valid_class_counts[0]/len(y_valid)*100:.3f}%)\")\n",
    "print(f\"  è©æ¬ºäº¤æ˜“: {valid_class_counts[1]:,} ({valid_class_counts[1]/len(y_valid)*100:.3f}%)\")\n",
    "\n",
    "print(f\"\\næ¸¬è©¦é›†é¡åˆ¥åˆ†å¸ƒ:\")\n",
    "test_class_counts = y_test.value_counts()\n",
    "print(f\"  æ­£å¸¸äº¤æ˜“: {test_class_counts[0]:,} ({test_class_counts[0]/len(y_test)*100:.3f}%)\")\n",
    "print(f\"  è©æ¬ºäº¤æ˜“: {test_class_counts[1]:,} ({test_class_counts[1]/len(y_test)*100:.3f}%)\")\n",
    "\n",
    "print(\"\\nğŸ’¡ V1~V28 æ˜¯ PCA å¾Œçš„ç‰¹å¾µï¼Œå·²ç¶“æ¨™æº–åŒ–éï¼Œä¸éœ€è¦é¡å¤– scaling\")\n",
    "\n",
    "# =========================\n",
    "# Step 4: è™•ç†é¡åˆ¥ä¸å¹³è¡¡\n",
    "# =========================\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"Step 4: è™•ç†é¡åˆ¥ä¸å¹³è¡¡\")\n",
    "print(\"=\"*80)\n",
    "print(\"\\nâš ï¸ é‡è¦: åªå°è¨“ç·´é›†åš resamplingï¼Œé©—è­‰é›†å’Œæ¸¬è©¦é›†ä¿æŒåŸæœ¬ä¸å¹³è¡¡\")\n",
    "\n",
    "# 4-1. ä½¿ç”¨ SMOTE\n",
    "print(\"\\nä½¿ç”¨ SMOTE è™•ç†è¨“ç·´é›†:\")\n",
    "smote = SMOTE(random_state=RANDOM_STATE)\n",
    "X_train_smote, y_train_smote = smote.fit_resample(X_train, y_train)\n",
    "\n",
    "print(f\"  åŸå§‹è¨“ç·´é›†: {len(y_train):,} ç­†\")\n",
    "print(f\"  SMOTE å¾Œ: {len(y_train_smote):,} ç­†\")\n",
    "print(f\"  è©æ¬ºäº¤æ˜“æ•¸é‡: {(y_train==1).sum()} â†’ {(y_train_smote==1).sum()}\")\n",
    "print(f\"  æ­£å¸¸äº¤æ˜“æ•¸é‡: {(y_train==0).sum()} â†’ {(y_train_smote==0).sum()}\")\n",
    "print(f\"  æ–°çš„æ¯”ä¾‹: {(y_train_smote==0).sum()}:{(y_train_smote==1).sum()} (å·²å¹³è¡¡)\")\n",
    "\n",
    "# =========================\n",
    "# Step 5: å»ºç«‹ Baseline æ¨¡å‹ + Optuna èª¿åƒ\n",
    "# =========================\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"Step 5: æ¨¡å‹è¨“ç·´\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# 5-1. Logistic Regression Baseline â­\n",
    "print(\"\\n\" + \"-\"*80)\n",
    "print(\"Step 5.1: Logistic Regression Baseline\")\n",
    "print(\"-\"*80)\n",
    "\n",
    "print(\"\\nè¨“ç·´ Logistic Regression ä½œç‚º baseline...\")\n",
    "\n",
    "# ä½¿ç”¨ SMOTE è¨“ç·´é›†\n",
    "lr_baseline = LogisticRegression(\n",
    "    max_iter=1000,\n",
    "    random_state=RANDOM_STATE,\n",
    "    n_jobs=-1\n",
    ")\n",
    "\n",
    "lr_baseline.fit(X_train_smote, y_train_smote)\n",
    "print(\"âœ“ Logistic Regression è¨“ç·´å®Œæˆ\")\n",
    "\n",
    "# åœ¨é©—è­‰é›†ä¸Šè©•ä¼°\n",
    "y_valid_pred_lr = lr_baseline.predict(X_valid)\n",
    "y_valid_proba_lr = lr_baseline.predict_proba(X_valid)[:, 1]\n",
    "\n",
    "lr_precision = precision_score(y_valid, y_valid_pred_lr, pos_label=1)\n",
    "lr_recall = recall_score(y_valid, y_valid_pred_lr, pos_label=1)\n",
    "lr_f1 = f1_score(y_valid, y_valid_pred_lr, pos_label=1)\n",
    "lr_roc_auc = roc_auc_score(y_valid, y_valid_proba_lr)\n",
    "\n",
    "print(\"\\né©—è­‰é›†æ€§èƒ½ (Logistic Regression Baseline):\")\n",
    "print(f\"  Precision: {lr_precision:.4f}\")\n",
    "print(f\"  Recall: {lr_recall:.4f}\")\n",
    "print(f\"  F1-Score: {lr_f1:.4f}\")\n",
    "print(f\"  ROC-AUC: {lr_roc_auc:.4f}\")\n",
    "\n",
    "# 5-2. Optuna èª¿åƒ - LightGBM\n",
    "print(\"\\n\" + \"-\"*80)\n",
    "print(\"Step 5.2: LightGBM è¶…åƒæ•¸èª¿æ•´\")\n",
    "print(\"-\"*80)\n",
    "\n",
    "def objective_lgb(trial):\n",
    "    \"\"\"LightGBM çš„ Optuna objective å‡½æ•¸\"\"\"\n",
    "    params = {\n",
    "        'n_estimators': trial.suggest_int('n_estimators', 100, 300),\n",
    "        'max_depth': trial.suggest_int('max_depth', 3, 10),\n",
    "        'learning_rate': trial.suggest_float('learning_rate', 0.01, 0.2),\n",
    "        'num_leaves': trial.suggest_int('num_leaves', 20, 100),\n",
    "        'min_child_samples': trial.suggest_int('min_child_samples', 10, 50),\n",
    "        'subsample': trial.suggest_float('subsample', 0.6, 1.0),\n",
    "        'colsample_bytree': trial.suggest_float('colsample_bytree', 0.6, 1.0),\n",
    "        'reg_alpha': trial.suggest_float('reg_alpha', 0.0, 1.0),\n",
    "        'reg_lambda': trial.suggest_float('reg_lambda', 0.0, 1.0),\n",
    "        'random_state': RANDOM_STATE,\n",
    "        'n_jobs': -1,\n",
    "        'verbose': -1\n",
    "    }\n",
    "    \n",
    "    model = lgb.LGBMClassifier(**params)\n",
    "    \n",
    "    # ä½¿ç”¨ StratifiedKFold äº¤å‰é©—è­‰\n",
    "    cv = StratifiedKFold(n_splits=3, shuffle=True, random_state=RANDOM_STATE)\n",
    "    f1_scores = []\n",
    "    \n",
    "    for train_idx, val_idx in cv.split(X_train_smote, y_train_smote):\n",
    "        X_tr = X_train_smote.iloc[train_idx]\n",
    "        X_val = X_train_smote.iloc[val_idx]\n",
    "        y_tr = y_train_smote.iloc[train_idx]\n",
    "        y_val = y_train_smote.iloc[val_idx]\n",
    "        \n",
    "        model.fit(X_tr, y_tr)\n",
    "        y_pred = model.predict(X_val)\n",
    "        f1 = f1_score(y_val, y_pred, pos_label=1)\n",
    "        f1_scores.append(f1)\n",
    "    \n",
    "    return np.mean(f1_scores)\n",
    "\n",
    "print(\"\\né–‹å§‹ Optuna å„ªåŒ–...\")\n",
    "optuna.logging.set_verbosity(optuna.logging.WARNING)\n",
    "\n",
    "study_lgb = optuna.create_study(direction='maximize')\n",
    "study_lgb.optimize(objective_lgb, n_trials=OPTUNA_TRIALS, show_progress_bar=True)\n",
    "\n",
    "print(f\"\\nâœ“ LightGBM èª¿åƒå®Œæˆ ({OPTUNA_TRIALS} trials)\")\n",
    "print(f\"  æœ€ä½³ F1-Score (CV): {study_lgb.best_value:.4f}\")\n",
    "print(f\"  æœ€ä½³åƒæ•¸:\")\n",
    "for key, value in study_lgb.best_params.items():\n",
    "    print(f\"    {key}: {value}\")\n",
    "\n",
    "# è¨“ç·´æœ€ä½³ LightGBM æ¨¡å‹ä¸¦åœ¨é©—è­‰é›†ä¸Šè©•ä¼°\n",
    "best_lgb = lgb.LGBMClassifier(**study_lgb.best_params, random_state=RANDOM_STATE, n_jobs=-1, verbose=-1)\n",
    "best_lgb.fit(X_train_smote, y_train_smote)\n",
    "y_valid_pred_lgb = best_lgb.predict(X_valid)\n",
    "y_valid_proba_lgb = best_lgb.predict_proba(X_valid)[:, 1]\n",
    "\n",
    "lgb_precision = precision_score(y_valid, y_valid_pred_lgb, pos_label=1)\n",
    "lgb_recall = recall_score(y_valid, y_valid_pred_lgb, pos_label=1)\n",
    "lgb_f1 = f1_score(y_valid, y_valid_pred_lgb, pos_label=1)\n",
    "lgb_roc_auc = roc_auc_score(y_valid, y_valid_proba_lgb)\n",
    "\n",
    "print(f\"\\né©—è­‰é›†æ€§èƒ½ (LightGBM):\")\n",
    "print(f\"  Precision: {lgb_precision:.4f}\")\n",
    "print(f\"  Recall: {lgb_recall:.4f}\")\n",
    "print(f\"  F1-Score: {lgb_f1:.4f}\")\n",
    "print(f\"  ROC-AUC: {lgb_roc_auc:.4f}\")\n",
    "\n",
    "# 5-3. Optuna èª¿åƒ - XGBoost\n",
    "print(\"\\n\" + \"-\"*80)\n",
    "print(\"Step 5.3: XGBoost è¶…åƒæ•¸èª¿æ•´\")\n",
    "print(\"-\"*80)\n",
    "\n",
    "def objective_xgb(trial):\n",
    "    \"\"\"XGBoost çš„ Optuna objective å‡½æ•¸\"\"\"\n",
    "    params = {\n",
    "        'n_estimators': trial.suggest_int('n_estimators', 100, 300),\n",
    "        'max_depth': trial.suggest_int('max_depth', 3, 10),\n",
    "        'learning_rate': trial.suggest_float('learning_rate', 0.01, 0.2),\n",
    "        'subsample': trial.suggest_float('subsample', 0.6, 1.0),\n",
    "        'colsample_bytree': trial.suggest_float('colsample_bytree', 0.6, 1.0),\n",
    "        'gamma': trial.suggest_float('gamma', 0.0, 1.0),\n",
    "        'reg_alpha': trial.suggest_float('reg_alpha', 0.0, 1.0),\n",
    "        'reg_lambda': trial.suggest_float('reg_lambda', 0.0, 1.0),\n",
    "        'random_state': RANDOM_STATE,\n",
    "        'n_jobs': -1,\n",
    "        'use_label_encoder': False,\n",
    "        'eval_metric': 'logloss'\n",
    "    }\n",
    "    \n",
    "    model = xgb.XGBClassifier(**params)\n",
    "    \n",
    "    cv = StratifiedKFold(n_splits=3, shuffle=True, random_state=RANDOM_STATE)\n",
    "    f1_scores = []\n",
    "    \n",
    "    for train_idx, val_idx in cv.split(X_train_smote, y_train_smote):\n",
    "        X_tr = X_train_smote.iloc[train_idx]\n",
    "        X_val = X_train_smote.iloc[val_idx]\n",
    "        y_tr = y_train_smote.iloc[train_idx]\n",
    "        y_val = y_train_smote.iloc[val_idx]\n",
    "        \n",
    "        model.fit(X_tr, y_tr)\n",
    "        y_pred = model.predict(X_val)\n",
    "        f1 = f1_score(y_val, y_pred, pos_label=1)\n",
    "        f1_scores.append(f1)\n",
    "    \n",
    "    return np.mean(f1_scores)\n",
    "\n",
    "print(\"\\né–‹å§‹ Optuna å„ªåŒ–...\")\n",
    "\n",
    "study_xgb = optuna.create_study(direction='maximize')\n",
    "study_xgb.optimize(objective_xgb, n_trials=OPTUNA_TRIALS, show_progress_bar=True)\n",
    "\n",
    "print(f\"\\nâœ“ XGBoost èª¿åƒå®Œæˆ ({OPTUNA_TRIALS} trials)\")\n",
    "print(f\"  æœ€ä½³ F1-Score (CV): {study_xgb.best_value:.4f}\")\n",
    "print(f\"  æœ€ä½³åƒæ•¸:\")\n",
    "for key, value in study_xgb.best_params.items():\n",
    "    print(f\"    {key}: {value}\")\n",
    "\n",
    "# è¨“ç·´æœ€ä½³ XGBoost æ¨¡å‹ä¸¦åœ¨é©—è­‰é›†ä¸Šè©•ä¼°\n",
    "best_xgb = xgb.XGBClassifier(**study_xgb.best_params, random_state=RANDOM_STATE, n_jobs=-1,\n",
    "                              use_label_encoder=False, eval_metric='logloss')\n",
    "best_xgb.fit(X_train_smote, y_train_smote)\n",
    "y_valid_pred_xgb = best_xgb.predict(X_valid)\n",
    "y_valid_proba_xgb = best_xgb.predict_proba(X_valid)[:, 1]\n",
    "\n",
    "xgb_precision = precision_score(y_valid, y_valid_pred_xgb, pos_label=1)\n",
    "xgb_recall = recall_score(y_valid, y_valid_pred_xgb, pos_label=1)\n",
    "xgb_f1 = f1_score(y_valid, y_valid_pred_xgb, pos_label=1)\n",
    "xgb_roc_auc = roc_auc_score(y_valid, y_valid_proba_xgb)\n",
    "\n",
    "print(f\"\\né©—è­‰é›†æ€§èƒ½ (XGBoost):\")\n",
    "print(f\"  Precision: {xgb_precision:.4f}\")\n",
    "print(f\"  Recall: {xgb_recall:.4f}\")\n",
    "print(f\"  F1-Score: {xgb_f1:.4f}\")\n",
    "print(f\"  ROC-AUC: {xgb_roc_auc:.4f}\")\n",
    "\n",
    "# =========================\n",
    "# Step 5.5: ä¸ç”¨ SMOTE çš„ç‰ˆæœ¬ï¼ˆä½¿ç”¨ class_weightï¼‰â­â­â­\n",
    "# =========================\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"Step 5.5: ä¸ç”¨ SMOTE çš„ç‰ˆæœ¬ï¼ˆä½¿ç”¨ class_weightï¼‰â­\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "print(\"\\nğŸ’¡ ç‚ºä»€éº¼å˜—è©¦ä¸ç”¨ SMOTEï¼Ÿ\")\n",
    "print(\"  - SMOTE æœƒã€Œæå‡ºå‡çš„è©æ¬ºæ¨£æœ¬ã€ï¼Œå¯èƒ½å­¸åˆ°äººé€ è¦å¾‹\")\n",
    "print(\"  - ç›´æ¥ç”¨åŸå§‹ä¸å¹³è¡¡åˆ†å¸ƒ + æ¬Šé‡ï¼Œæ¨¡å‹åœ¨çœŸå¯¦æ¯”ä¾‹ä¸‹å­¸ç¿’\")\n",
    "print(\"  - å¯¦å‹™ä¸­ï¼Œclass_weight ç‰ˆæœ¬å¸¸å¸¸è¡¨ç¾æ›´ç©©å®š\")\n",
    "\n",
    "# è¨ˆç®— scale_pos_weight\n",
    "neg_count = (y_train == 0).sum()\n",
    "pos_count = (y_train == 1).sum()\n",
    "scale_pos_weight = neg_count / pos_count\n",
    "\n",
    "print(f\"\\nè³‡æ–™çµ±è¨ˆ:\")\n",
    "print(f\"  æ­£å¸¸äº¤æ˜“ (Class=0): {neg_count:,}\")\n",
    "print(f\"  è©æ¬ºäº¤æ˜“ (Class=1): {pos_count:,}\")\n",
    "print(f\"  scale_pos_weight: {scale_pos_weight:.2f}\")\n",
    "\n",
    "# 5.5.1 LightGBM with class_weight\n",
    "print(\"\\n\" + \"-\"*80)\n",
    "print(\"Step 5.5.1: LightGBM (class_weight='balanced')\")\n",
    "print(\"-\"*80)\n",
    "\n",
    "def objective_lgb_weighted(trial):\n",
    "    \"\"\"LightGBM with class_weight\"\"\"\n",
    "    params = {\n",
    "        'n_estimators': trial.suggest_int('n_estimators', 100, 300),\n",
    "        'max_depth': trial.suggest_int('max_depth', 3, 10),\n",
    "        'learning_rate': trial.suggest_float('learning_rate', 0.01, 0.2),\n",
    "        'num_leaves': trial.suggest_int('num_leaves', 20, 100),\n",
    "        'min_child_samples': trial.suggest_int('min_child_samples', 10, 50),\n",
    "        'subsample': trial.suggest_float('subsample', 0.6, 1.0),\n",
    "        'colsample_bytree': trial.suggest_float('colsample_bytree', 0.6, 1.0),\n",
    "        'reg_alpha': trial.suggest_float('reg_alpha', 0.0, 1.0),\n",
    "        'reg_lambda': trial.suggest_float('reg_lambda', 0.0, 1.0),\n",
    "        'class_weight': 'balanced',  # â­ é—œéµå·®ç•°\n",
    "        'random_state': RANDOM_STATE,\n",
    "        'n_jobs': -1,\n",
    "        'verbose': -1\n",
    "    }\n",
    "    \n",
    "    model = lgb.LGBMClassifier(**params)\n",
    "    \n",
    "    cv = StratifiedKFold(n_splits=3, shuffle=True, random_state=RANDOM_STATE)\n",
    "    f1_scores = []\n",
    "    \n",
    "    for train_idx, val_idx in cv.split(X_train, y_train):  # â­ ç”¨åŸå§‹ä¸å¹³è¡¡è³‡æ–™\n",
    "        X_tr = X_train.iloc[train_idx]\n",
    "        X_val = X_train.iloc[val_idx]\n",
    "        y_tr = y_train.iloc[train_idx]\n",
    "        y_val = y_train.iloc[val_idx]\n",
    "        \n",
    "        model.fit(X_tr, y_tr)\n",
    "        y_pred = model.predict(X_val)\n",
    "        f1 = f1_score(y_val, y_pred, pos_label=1)\n",
    "        f1_scores.append(f1)\n",
    "    \n",
    "    return np.mean(f1_scores)\n",
    "\n",
    "print(\"\\né–‹å§‹ Optuna å„ªåŒ–ï¼ˆä¸ç”¨ SMOTEï¼Œä½¿ç”¨ class_weightï¼‰...\")\n",
    "study_lgb_weighted = optuna.create_study(direction='maximize')\n",
    "study_lgb_weighted.optimize(objective_lgb_weighted, n_trials=OPTUNA_TRIALS, show_progress_bar=True)\n",
    "\n",
    "print(f\"\\nâœ“ LightGBM (class_weight) èª¿åƒå®Œæˆ ({OPTUNA_TRIALS} trials)\")\n",
    "print(f\"  æœ€ä½³ F1-Score (CV): {study_lgb_weighted.best_value:.4f}\")\n",
    "\n",
    "# è¨“ç·´æœ€ä½³æ¨¡å‹\n",
    "best_lgb_weighted = lgb.LGBMClassifier(**study_lgb_weighted.best_params, \n",
    "                                        class_weight='balanced',\n",
    "                                        random_state=RANDOM_STATE, \n",
    "                                        n_jobs=-1, \n",
    "                                        verbose=-1)\n",
    "best_lgb_weighted.fit(X_train, y_train)  # â­ ç”¨åŸå§‹ä¸å¹³è¡¡è³‡æ–™\n",
    "y_valid_pred_lgb_weighted = best_lgb_weighted.predict(X_valid)\n",
    "y_valid_proba_lgb_weighted = best_lgb_weighted.predict_proba(X_valid)[:, 1]\n",
    "\n",
    "lgb_weighted_precision = precision_score(y_valid, y_valid_pred_lgb_weighted, pos_label=1)\n",
    "lgb_weighted_recall = recall_score(y_valid, y_valid_pred_lgb_weighted, pos_label=1)\n",
    "lgb_weighted_f1 = f1_score(y_valid, y_valid_pred_lgb_weighted, pos_label=1)\n",
    "lgb_weighted_roc_auc = roc_auc_score(y_valid, y_valid_proba_lgb_weighted)\n",
    "\n",
    "print(f\"\\né©—è­‰é›†æ€§èƒ½ (LightGBM with class_weight):\")\n",
    "print(f\"  Precision: {lgb_weighted_precision:.4f}\")\n",
    "print(f\"  Recall: {lgb_weighted_recall:.4f}\")\n",
    "print(f\"  F1-Score: {lgb_weighted_f1:.4f}\")\n",
    "print(f\"  ROC-AUC: {lgb_weighted_roc_auc:.4f}\")\n",
    "\n",
    "# 5.5.2 XGBoost with scale_pos_weight\n",
    "print(\"\\n\" + \"-\"*80)\n",
    "print(\"Step 5.5.2: XGBoost (scale_pos_weight)\")\n",
    "print(\"-\"*80)\n",
    "\n",
    "def objective_xgb_weighted(trial):\n",
    "    \"\"\"XGBoost with scale_pos_weight\"\"\"\n",
    "    params = {\n",
    "        'n_estimators': trial.suggest_int('n_estimators', 100, 300),\n",
    "        'max_depth': trial.suggest_int('max_depth', 3, 10),\n",
    "        'learning_rate': trial.suggest_float('learning_rate', 0.01, 0.2),\n",
    "        'subsample': trial.suggest_float('subsample', 0.6, 1.0),\n",
    "        'colsample_bytree': trial.suggest_float('colsample_bytree', 0.6, 1.0),\n",
    "        'gamma': trial.suggest_float('gamma', 0.0, 1.0),\n",
    "        'reg_alpha': trial.suggest_float('reg_alpha', 0.0, 1.0),\n",
    "        'reg_lambda': trial.suggest_float('reg_lambda', 0.0, 1.0),\n",
    "        'scale_pos_weight': scale_pos_weight,  # â­ é—œéµå·®ç•°\n",
    "        'random_state': RANDOM_STATE,\n",
    "        'n_jobs': -1,\n",
    "        'use_label_encoder': False,\n",
    "        'eval_metric': 'logloss'\n",
    "    }\n",
    "    \n",
    "    model = xgb.XGBClassifier(**params)\n",
    "    \n",
    "    cv = StratifiedKFold(n_splits=3, shuffle=True, random_state=RANDOM_STATE)\n",
    "    f1_scores = []\n",
    "    \n",
    "    for train_idx, val_idx in cv.split(X_train, y_train):  # â­ ç”¨åŸå§‹ä¸å¹³è¡¡è³‡æ–™\n",
    "        X_tr = X_train.iloc[train_idx]\n",
    "        X_val = X_train.iloc[val_idx]\n",
    "        y_tr = y_train.iloc[train_idx]\n",
    "        y_val = y_train.iloc[val_idx]\n",
    "        \n",
    "        model.fit(X_tr, y_tr)\n",
    "        y_pred = model.predict(X_val)\n",
    "        f1 = f1_score(y_val, y_pred, pos_label=1)\n",
    "        f1_scores.append(f1)\n",
    "    \n",
    "    return np.mean(f1_scores)\n",
    "\n",
    "print(\"\\né–‹å§‹ Optuna å„ªåŒ–ï¼ˆä¸ç”¨ SMOTEï¼Œä½¿ç”¨ scale_pos_weightï¼‰...\")\n",
    "study_xgb_weighted = optuna.create_study(direction='maximize')\n",
    "study_xgb_weighted.optimize(objective_xgb_weighted, n_trials=OPTUNA_TRIALS, show_progress_bar=True)\n",
    "\n",
    "print(f\"\\nâœ“ XGBoost (scale_pos_weight) èª¿åƒå®Œæˆ ({OPTUNA_TRIALS} trials)\")\n",
    "print(f\"  æœ€ä½³ F1-Score (CV): {study_xgb_weighted.best_value:.4f}\")\n",
    "\n",
    "# è¨“ç·´æœ€ä½³æ¨¡å‹\n",
    "best_xgb_weighted = xgb.XGBClassifier(**study_xgb_weighted.best_params,\n",
    "                                       scale_pos_weight=scale_pos_weight,\n",
    "                                       random_state=RANDOM_STATE,\n",
    "                                       n_jobs=-1,\n",
    "                                       use_label_encoder=False,\n",
    "                                       eval_metric='logloss')\n",
    "best_xgb_weighted.fit(X_train, y_train)  # â­ ç”¨åŸå§‹ä¸å¹³è¡¡è³‡æ–™\n",
    "y_valid_pred_xgb_weighted = best_xgb_weighted.predict(X_valid)\n",
    "y_valid_proba_xgb_weighted = best_xgb_weighted.predict_proba(X_valid)[:, 1]\n",
    "\n",
    "xgb_weighted_precision = precision_score(y_valid, y_valid_pred_xgb_weighted, pos_label=1)\n",
    "xgb_weighted_recall = recall_score(y_valid, y_valid_pred_xgb_weighted, pos_label=1)\n",
    "xgb_weighted_f1 = f1_score(y_valid, y_valid_pred_xgb_weighted, pos_label=1)\n",
    "xgb_weighted_roc_auc = roc_auc_score(y_valid, y_valid_proba_xgb_weighted)\n",
    "\n",
    "print(f\"\\né©—è­‰é›†æ€§èƒ½ (XGBoost with scale_pos_weight):\")\n",
    "print(f\"  Precision: {xgb_weighted_precision:.4f}\")\n",
    "print(f\"  Recall: {xgb_weighted_recall:.4f}\")\n",
    "print(f\"  F1-Score: {xgb_weighted_f1:.4f}\")\n",
    "print(f\"  ROC-AUC: {xgb_weighted_roc_auc:.4f}\")\n",
    "\n",
    "# 5-6. æ¯”è¼ƒæ‰€æœ‰æ¨¡å‹ï¼ˆåŒ…æ‹¬ SMOTE å’Œ class_weight ç‰ˆæœ¬ï¼‰\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"æ¨¡å‹æ¯”è¼ƒ (é©—è­‰é›†):\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "print(\"\\nğŸ“Š A/B æ¯”è¼ƒï¼šSMOTE vs class_weight\")\n",
    "print(\"-\" * 80)\n",
    "\n",
    "# å‰µå»ºå®Œæ•´çš„æ¯”è¼ƒè¡¨æ ¼\n",
    "results_comparison = pd.DataFrame({\n",
    "    'Model': [\n",
    "        'Logistic Regression', \n",
    "        'LightGBM (SMOTE)', \n",
    "        'XGBoost (SMOTE)',\n",
    "        'LightGBM (class_weight) â­',\n",
    "        'XGBoost (scale_pos_weight) â­'\n",
    "    ],\n",
    "    'Method': [\n",
    "        'Baseline',\n",
    "        'SMOTE',\n",
    "        'SMOTE',\n",
    "        'class_weight',\n",
    "        'scale_pos_weight'\n",
    "    ],\n",
    "    'Precision': [\n",
    "        lr_precision, \n",
    "        lgb_precision, \n",
    "        xgb_precision,\n",
    "        lgb_weighted_precision,\n",
    "        xgb_weighted_precision\n",
    "    ],\n",
    "    'Recall': [\n",
    "        lr_recall, \n",
    "        lgb_recall, \n",
    "        xgb_recall,\n",
    "        lgb_weighted_recall,\n",
    "        xgb_weighted_recall\n",
    "    ],\n",
    "    'F1-Score': [\n",
    "        lr_f1, \n",
    "        lgb_f1, \n",
    "        xgb_f1,\n",
    "        lgb_weighted_f1,\n",
    "        xgb_weighted_f1\n",
    "    ],\n",
    "    'ROC-AUC': [\n",
    "        lr_roc_auc, \n",
    "        lgb_roc_auc, \n",
    "        xgb_roc_auc,\n",
    "        lgb_weighted_roc_auc,\n",
    "        xgb_weighted_roc_auc\n",
    "    ]\n",
    "})\n",
    "\n",
    "print(\"\\n\" + results_comparison.to_string(index=False))\n",
    "\n",
    "# A/B æ¯”è¼ƒåˆ†æ\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"A/B æ¯”è¼ƒåˆ†æï¼š\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "print(f\"\\nLightGBM:\")\n",
    "print(f\"  SMOTE ç‰ˆæœ¬:        F1 = {lgb_f1:.4f}\")\n",
    "print(f\"  class_weight ç‰ˆæœ¬: F1 = {lgb_weighted_f1:.4f}\")\n",
    "if lgb_weighted_f1 > lgb_f1:\n",
    "    print(f\"  â†’ class_weight ç‰ˆæœ¬è¼ƒå¥½ï¼ (+{(lgb_weighted_f1 - lgb_f1)*100:.2f}%)\")\n",
    "else:\n",
    "    print(f\"  â†’ SMOTE ç‰ˆæœ¬è¼ƒå¥½ï¼ (+{(lgb_f1 - lgb_weighted_f1)*100:.2f}%)\")\n",
    "\n",
    "print(f\"\\nXGBoost:\")\n",
    "print(f\"  SMOTE ç‰ˆæœ¬:             F1 = {xgb_f1:.4f}\")\n",
    "print(f\"  scale_pos_weight ç‰ˆæœ¬: F1 = {xgb_weighted_f1:.4f}\")\n",
    "if xgb_weighted_f1 > xgb_f1:\n",
    "    print(f\"  â†’ scale_pos_weight ç‰ˆæœ¬è¼ƒå¥½ï¼ (+{(xgb_weighted_f1 - xgb_f1)*100:.2f}%)\")\n",
    "else:\n",
    "    print(f\"  â†’ SMOTE ç‰ˆæœ¬è¼ƒå¥½ï¼ (+{(xgb_f1 - xgb_weighted_f1)*100:.2f}%)\")\n",
    "\n",
    "# è¦–è¦ºåŒ–æ¨¡å‹æ¯”è¼ƒï¼ˆæ›´æ–°ç‚º 5 å€‹æ¨¡å‹ï¼‰\n",
    "fig, axes = plt.subplots(2, 2, figsize=(16, 12))\n",
    "\n",
    "model_names_short = ['LR', 'LGB\\n(SMOTE)', 'XGB\\n(SMOTE)', 'LGB\\n(weight)', 'XGB\\n(weight)']\n",
    "colors = ['#3498db', '#e74c3c', '#2ecc71', '#f39c12', '#9b59b6']\n",
    "\n",
    "# Precision æ¯”è¼ƒ\n",
    "axes[0, 0].bar(model_names_short, results_comparison['Precision'], color=colors)\n",
    "axes[0, 0].set_title('Precision Comparison (All Models)', fontsize=14, fontweight='bold')\n",
    "axes[0, 0].set_ylabel('Precision', fontsize=12)\n",
    "axes[0, 0].set_ylim([0.7, 1.0])\n",
    "axes[0, 0].grid(alpha=0.3, axis='y')\n",
    "for i, v in enumerate(results_comparison['Precision']):\n",
    "    axes[0, 0].text(i, v + 0.01, f'{v:.4f}', ha='center', fontweight='bold', fontsize=9)\n",
    "\n",
    "# Recall æ¯”è¼ƒ\n",
    "axes[0, 1].bar(model_names_short, results_comparison['Recall'], color=colors)\n",
    "axes[0, 1].set_title('Recall Comparison (All Models)', fontsize=14, fontweight='bold')\n",
    "axes[0, 1].set_ylabel('Recall', fontsize=12)\n",
    "axes[0, 1].set_ylim([0.7, 1.0])\n",
    "axes[0, 1].grid(alpha=0.3, axis='y')\n",
    "for i, v in enumerate(results_comparison['Recall']):\n",
    "    axes[0, 1].text(i, v + 0.01, f'{v:.4f}', ha='center', fontweight='bold', fontsize=9)\n",
    "\n",
    "# F1-Score æ¯”è¼ƒ\n",
    "axes[1, 0].bar(model_names_short, results_comparison['F1-Score'], color=colors)\n",
    "axes[1, 0].set_title('F1-Score Comparison (All Models)', fontsize=14, fontweight='bold')\n",
    "axes[1, 0].set_ylabel('F1-Score', fontsize=12)\n",
    "axes[1, 0].set_ylim([0.7, 1.0])\n",
    "axes[1, 0].grid(alpha=0.3, axis='y')\n",
    "for i, v in enumerate(results_comparison['F1-Score']):\n",
    "    axes[1, 0].text(i, v + 0.01, f'{v:.4f}', ha='center', fontweight='bold', fontsize=9)\n",
    "\n",
    "# ROC-AUC æ¯”è¼ƒ\n",
    "axes[1, 1].bar(model_names_short, results_comparison['ROC-AUC'], color=colors)\n",
    "axes[1, 1].set_title('ROC-AUC Comparison (All Models)', fontsize=14, fontweight='bold')\n",
    "axes[1, 1].set_ylabel('ROC-AUC', fontsize=12)\n",
    "axes[1, 1].set_ylim([0.7, 1.0])\n",
    "axes[1, 1].grid(alpha=0.3, axis='y')\n",
    "for i, v in enumerate(results_comparison['ROC-AUC']):\n",
    "    axes[1, 1].text(i, v + 0.01, f'{v:.4f}', ha='center', fontweight='bold', fontsize=9)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(os.path.join(OUTPUT_DIR, '03_model_comparison_all.png'), dpi=300, bbox_inches='tight')\n",
    "plt.close()\n",
    "print(f\"\\nâœ“ å®Œæ•´æ¨¡å‹æ¯”è¼ƒåœ–å·²ä¿å­˜: {OUTPUT_DIR}/03_model_comparison_all.png\")\n",
    "\n",
    "# é¸æ“‡æœ€ä½³æ¨¡å‹ï¼ˆå¾æ‰€æœ‰ 5 å€‹æ¨¡å‹ä¸­ï¼‰\n",
    "all_f1_scores = [lr_f1, lgb_f1, xgb_f1, lgb_weighted_f1, xgb_weighted_f1]\n",
    "all_models = [\n",
    "    lr_baseline, \n",
    "    best_lgb, \n",
    "    best_xgb, \n",
    "    best_lgb_weighted, \n",
    "    best_xgb_weighted\n",
    "]\n",
    "all_model_names = [\n",
    "    'Logistic Regression',\n",
    "    'LightGBM (SMOTE)',\n",
    "    'XGBoost (SMOTE)',\n",
    "    'LightGBM (class_weight)',\n",
    "    'XGBoost (scale_pos_weight)'\n",
    "]\n",
    "\n",
    "best_model_idx = np.argmax(all_f1_scores)\n",
    "best_model_name = all_model_names[best_model_idx]\n",
    "final_model = all_models[best_model_idx]\n",
    "\n",
    "print(f\"\\nğŸ† é¸æ“‡ {best_model_name} ä½œç‚ºæœ€ä½³æ¨¡å‹\")\n",
    "print(f\"   é©—è­‰é›† F1-Score: {all_f1_scores[best_model_idx]:.4f}\")\n",
    "\n",
    "# =========================\n",
    "# Step 5.7: Ensemble æ¨¡å‹ï¼ˆLightGBM + XGBoostï¼‰â­â­â­\n",
    "# =========================\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"Step 5.7: Ensemble æ¨¡å‹ï¼ˆLightGBM + XGBoostï¼‰â­\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "print(\"\\nğŸ’¡ ç‚ºä»€éº¼ä½¿ç”¨ Ensembleï¼Ÿ\")\n",
    "print(\"  - çµåˆå¤šå€‹æ¨¡å‹çš„å„ªå‹¢\")\n",
    "print(\"  - é™ä½å–®ä¸€æ¨¡å‹çš„åå·®\")\n",
    "print(\"  - é€šå¸¸èƒ½ç²å¾—æ›´ç©©å®šçš„é æ¸¬\")\n",
    "\n",
    "# 5.7.1 æº–å‚™ Ensemble çš„åŸºç¤æ¨¡å‹\n",
    "print(\"\\n\" + \"-\"*80)\n",
    "print(\"Step 5.7.1: æº–å‚™ Ensemble åŸºç¤æ¨¡å‹\")\n",
    "print(\"-\"*80)\n",
    "\n",
    "# é¸æ“‡è¡¨ç¾æœ€å¥½çš„ LightGBM å’Œ XGBoost\n",
    "if lgb_weighted_f1 > lgb_f1:\n",
    "    ensemble_lgb = best_lgb_weighted\n",
    "    lgb_ensemble_name = \"LightGBM (class_weight)\"\n",
    "    lgb_ensemble_f1 = lgb_weighted_f1\n",
    "    y_valid_proba_lgb_ensemble = y_valid_proba_lgb_weighted\n",
    "else:\n",
    "    ensemble_lgb = best_lgb\n",
    "    lgb_ensemble_name = \"LightGBM (SMOTE)\"\n",
    "    lgb_ensemble_f1 = lgb_f1\n",
    "    y_valid_proba_lgb_ensemble = y_valid_proba_lgb\n",
    "\n",
    "if xgb_weighted_f1 > xgb_f1:\n",
    "    ensemble_xgb = best_xgb_weighted\n",
    "    xgb_ensemble_name = \"XGBoost (scale_pos_weight)\"\n",
    "    xgb_ensemble_f1 = xgb_weighted_f1\n",
    "    y_valid_proba_xgb_ensemble = y_valid_proba_xgb_weighted\n",
    "else:\n",
    "    ensemble_xgb = best_xgb\n",
    "    xgb_ensemble_name = \"XGBoost (SMOTE)\"\n",
    "    xgb_ensemble_f1 = xgb_f1\n",
    "    y_valid_proba_xgb_ensemble = y_valid_proba_xgb\n",
    "\n",
    "print(f\"\\nEnsemble åŸºç¤æ¨¡å‹:\")\n",
    "print(f\"  1. {lgb_ensemble_name} (F1={lgb_ensemble_f1:.4f})\")\n",
    "print(f\"  2. {xgb_ensemble_name} (F1={xgb_ensemble_f1:.4f})\")\n",
    "\n",
    "# 5.7.2 æ–¹æ³• 1: Simple Average Ensemble\n",
    "print(\"\\n\" + \"-\"*80)\n",
    "print(\"Step 5.7.2: æ–¹æ³• 1 - Simple Average Ensemble\")\n",
    "print(\"-\"*80)\n",
    "\n",
    "print(\"\\nè¨ˆç®—é æ¸¬æ©Ÿç‡çš„ç°¡å–®å¹³å‡...\")\n",
    "y_valid_proba_avg = (y_valid_proba_lgb_ensemble + y_valid_proba_xgb_ensemble) / 2\n",
    "y_valid_pred_avg = (y_valid_proba_avg >= 0.5).astype(int)\n",
    "\n",
    "avg_precision = precision_score(y_valid, y_valid_pred_avg, pos_label=1)\n",
    "avg_recall = recall_score(y_valid, y_valid_pred_avg, pos_label=1)\n",
    "avg_f1 = f1_score(y_valid, y_valid_pred_avg, pos_label=1)\n",
    "avg_roc_auc = roc_auc_score(y_valid, y_valid_proba_avg)\n",
    "\n",
    "print(f\"\\né©—è­‰é›†æ€§èƒ½ (Simple Average Ensemble):\")\n",
    "print(f\"  Precision: {avg_precision:.4f}\")\n",
    "print(f\"  Recall: {avg_recall:.4f}\")\n",
    "print(f\"  F1-Score: {avg_f1:.4f}\")\n",
    "print(f\"  ROC-AUC: {avg_roc_auc:.4f}\")\n",
    "\n",
    "# 5.7.3 æ–¹æ³• 2: Weighted Average Ensemble (åŸºæ–¼é©—è­‰é›† F1)\n",
    "print(\"\\n\" + \"-\"*80)\n",
    "print(\"Step 5.7.3: æ–¹æ³• 2 - Weighted Average Ensemble (åŸºæ–¼ F1-Score)\")\n",
    "print(\"-\"*80)\n",
    "\n",
    "# è¨ˆç®—æ¬Šé‡ï¼ˆåŸºæ–¼é©—è­‰é›† F1-Scoreï¼‰\n",
    "total_f1 = lgb_ensemble_f1 + xgb_ensemble_f1\n",
    "lgb_weight = lgb_ensemble_f1 / total_f1\n",
    "xgb_weight = xgb_ensemble_f1 / total_f1\n",
    "\n",
    "print(f\"\\næ¬Šé‡åˆ†é…:\")\n",
    "print(f\"  LightGBM æ¬Šé‡: {lgb_weight:.4f} (åŸºæ–¼ F1={lgb_ensemble_f1:.4f})\")\n",
    "print(f\"  XGBoost æ¬Šé‡: {xgb_weight:.4f} (åŸºæ–¼ F1={xgb_ensemble_f1:.4f})\")\n",
    "\n",
    "y_valid_proba_weighted = (lgb_weight * y_valid_proba_lgb_ensemble + \n",
    "                          xgb_weight * y_valid_proba_xgb_ensemble)\n",
    "y_valid_pred_weighted = (y_valid_proba_weighted >= 0.5).astype(int)\n",
    "\n",
    "weighted_precision = precision_score(y_valid, y_valid_pred_weighted, pos_label=1)\n",
    "weighted_recall = recall_score(y_valid, y_valid_pred_weighted, pos_label=1)\n",
    "weighted_f1 = f1_score(y_valid, y_valid_pred_weighted, pos_label=1)\n",
    "weighted_roc_auc = roc_auc_score(y_valid, y_valid_proba_weighted)\n",
    "\n",
    "print(f\"\\né©—è­‰é›†æ€§èƒ½ (Weighted Average Ensemble):\")\n",
    "print(f\"  Precision: {weighted_precision:.4f}\")\n",
    "print(f\"  Recall: {weighted_recall:.4f}\")\n",
    "print(f\"  F1-Score: {weighted_f1:.4f}\")\n",
    "print(f\"  ROC-AUC: {weighted_roc_auc:.4f}\")\n",
    "\n",
    "# 5.7.4 æ–¹æ³• 3: Stacking Ensemble\n",
    "print(\"\\n\" + \"-\"*80)\n",
    "print(\"Step 5.7.4: æ–¹æ³• 3 - Stacking Ensemble (Meta-learner)\")\n",
    "print(\"-\"*80)\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression as LR\n",
    "\n",
    "print(\"\\nè¨“ç·´ Stacking æ¨¡å‹...\")\n",
    "print(\"  - åŸºç¤æ¨¡å‹: LightGBM + XGBoost\")\n",
    "print(\"  - Meta-learner: Logistic Regression\")\n",
    "\n",
    "# å‰µå»º stacking ç‰¹å¾µï¼ˆä½¿ç”¨é©—è­‰é›†çš„é æ¸¬æ©Ÿç‡ï¼‰\n",
    "X_valid_stack = np.column_stack([y_valid_proba_lgb_ensemble, y_valid_proba_xgb_ensemble])\n",
    "\n",
    "# ä½¿ç”¨è¨“ç·´é›†ç”Ÿæˆé æ¸¬ä¾†è¨“ç·´ meta-learner\n",
    "# âš ï¸ é—œéµï¼šå…©å€‹æ¨¡å‹éƒ½åœ¨ã€ŒåŸå§‹è¨“ç·´é›† X_trainã€ä¸Šç”Ÿæˆé æ¸¬\n",
    "print(\"\\n  ç”Ÿæˆè¨“ç·´é›†çš„é æ¸¬æ©Ÿç‡...\")\n",
    "\n",
    "# LightGBM åœ¨åŸå§‹è¨“ç·´é›†ä¸Šçš„é æ¸¬\n",
    "y_train_proba_lgb_meta = ensemble_lgb.predict_proba(X_train)[:, 1]\n",
    "print(f\"    LightGBM é æ¸¬: {len(y_train_proba_lgb_meta)} ç­†\")\n",
    "\n",
    "# XGBoost åœ¨åŸå§‹è¨“ç·´é›†ä¸Šçš„é æ¸¬\n",
    "y_train_proba_xgb_meta = ensemble_xgb.predict_proba(X_train)[:, 1]\n",
    "print(f\"    XGBoost é æ¸¬: {len(y_train_proba_xgb_meta)} ç­†\")\n",
    "\n",
    "# è¨“ç·´é›†çš„ stacking ç‰¹å¾µ\n",
    "X_train_stack = np.column_stack([y_train_proba_lgb_meta, y_train_proba_xgb_meta])\n",
    "print(f\"  âœ“ Stacking ç‰¹å¾µçŸ©é™£: {X_train_stack.shape}\")\n",
    "\n",
    "# è¨“ç·´ meta-learnerï¼ˆä½¿ç”¨åŸå§‹è¨“ç·´é›†çš„æ¨™ç±¤ï¼‰\n",
    "meta_learner = LR(max_iter=1000, random_state=RANDOM_STATE)\n",
    "meta_learner.fit(X_train_stack, y_train)  # â­ ä½¿ç”¨åŸå§‹ y_train\n",
    "print(\"âœ“ Meta-learner è¨“ç·´å®Œæˆ\")\n",
    "\n",
    "# åœ¨é©—è­‰é›†ä¸Šé æ¸¬\n",
    "y_valid_proba_stack = meta_learner.predict_proba(X_valid_stack)[:, 1]\n",
    "y_valid_pred_stack = meta_learner.predict(X_valid_stack)\n",
    "\n",
    "stack_precision = precision_score(y_valid, y_valid_pred_stack, pos_label=1)\n",
    "stack_recall = recall_score(y_valid, y_valid_pred_stack, pos_label=1)\n",
    "stack_f1 = f1_score(y_valid, y_valid_pred_stack, pos_label=1)\n",
    "stack_roc_auc = roc_auc_score(y_valid, y_valid_proba_stack)\n",
    "\n",
    "print(f\"\\né©—è­‰é›†æ€§èƒ½ (Stacking Ensemble):\")\n",
    "print(f\"  Precision: {stack_precision:.4f}\")\n",
    "print(f\"  Recall: {stack_recall:.4f}\")\n",
    "print(f\"  F1-Score: {stack_f1:.4f}\")\n",
    "print(f\"  ROC-AUC: {stack_roc_auc:.4f}\")\n",
    "\n",
    "# 5.7.5 Ensemble æ–¹æ³•æ¯”è¼ƒ\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"Ensemble æ–¹æ³•æ¯”è¼ƒ:\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "ensemble_comparison = pd.DataFrame({\n",
    "    'Method': [\n",
    "        lgb_ensemble_name,\n",
    "        xgb_ensemble_name,\n",
    "        'Simple Average',\n",
    "        'Weighted Average',\n",
    "        'Stacking'\n",
    "    ],\n",
    "    'Type': [\n",
    "        'Single Model',\n",
    "        'Single Model',\n",
    "        'Ensemble',\n",
    "        'Ensemble',\n",
    "        'Ensemble'\n",
    "    ],\n",
    "    'F1-Score': [\n",
    "        lgb_ensemble_f1,\n",
    "        xgb_ensemble_f1,\n",
    "        avg_f1,\n",
    "        weighted_f1,\n",
    "        stack_f1\n",
    "    ],\n",
    "    'Precision': [\n",
    "        precision_score(y_valid, ensemble_lgb.predict(X_valid if 'class_weight' in lgb_ensemble_name else X_valid), pos_label=1),\n",
    "        precision_score(y_valid, ensemble_xgb.predict(X_valid if 'scale_pos_weight' in xgb_ensemble_name else X_valid), pos_label=1),\n",
    "        avg_precision,\n",
    "        weighted_precision,\n",
    "        stack_precision\n",
    "    ],\n",
    "    'Recall': [\n",
    "        recall_score(y_valid, ensemble_lgb.predict(X_valid if 'class_weight' in lgb_ensemble_name else X_valid), pos_label=1),\n",
    "        recall_score(y_valid, ensemble_xgb.predict(X_valid if 'scale_pos_weight' in xgb_ensemble_name else X_valid), pos_label=1),\n",
    "        avg_recall,\n",
    "        weighted_recall,\n",
    "        stack_recall\n",
    "    ],\n",
    "    'ROC-AUC': [\n",
    "        roc_auc_score(y_valid, y_valid_proba_lgb_ensemble),\n",
    "        roc_auc_score(y_valid, y_valid_proba_xgb_ensemble),\n",
    "        avg_roc_auc,\n",
    "        weighted_roc_auc,\n",
    "        stack_roc_auc\n",
    "    ]\n",
    "})\n",
    "\n",
    "print(\"\\n\" + ensemble_comparison.to_string(index=False))\n",
    "\n",
    "# é¸æ“‡æœ€ä½³ Ensemble æ–¹æ³•\n",
    "ensemble_f1_scores = [avg_f1, weighted_f1, stack_f1]\n",
    "ensemble_methods = ['Simple Average', 'Weighted Average', 'Stacking']\n",
    "best_ensemble_idx = np.argmax(ensemble_f1_scores)\n",
    "best_ensemble_method = ensemble_methods[best_ensemble_idx]\n",
    "best_ensemble_f1 = ensemble_f1_scores[best_ensemble_idx]\n",
    "\n",
    "print(f\"\\nğŸ† æœ€ä½³ Ensemble æ–¹æ³•: {best_ensemble_method}\")\n",
    "print(f\"   F1-Score: {best_ensemble_f1:.4f}\")\n",
    "\n",
    "# èˆ‡å–®ä¸€æ¨¡å‹æ¯”è¼ƒ\n",
    "print(f\"\\nEnsemble vs æœ€ä½³å–®ä¸€æ¨¡å‹:\")\n",
    "print(f\"  æœ€ä½³å–®ä¸€æ¨¡å‹: {best_model_name} (F1={all_f1_scores[best_model_idx]:.4f})\")\n",
    "print(f\"  æœ€ä½³ Ensemble: {best_ensemble_method} (F1={best_ensemble_f1:.4f})\")\n",
    "\n",
    "if best_ensemble_f1 > all_f1_scores[best_model_idx]:\n",
    "    improvement = (best_ensemble_f1 - all_f1_scores[best_model_idx]) * 100\n",
    "    print(f\"  â†’ Ensemble è¼ƒå¥½ï¼æå‡ {improvement:.2f}% âœ…\")\n",
    "    \n",
    "    # æ›´æ–°æœ€ä½³æ¨¡å‹ç‚º Ensemble\n",
    "    best_model_name = f\"Ensemble ({best_ensemble_method})\"\n",
    "    \n",
    "    # æ ¹æ“šé¸æ“‡çš„ ensemble æ–¹æ³•è¨­å®šé æ¸¬å‡½æ•¸\n",
    "    if best_ensemble_method == 'Simple Average':\n",
    "        final_ensemble_proba = y_valid_proba_avg\n",
    "    elif best_ensemble_method == 'Weighted Average':\n",
    "        final_ensemble_proba = y_valid_proba_weighted\n",
    "    else:  # Stacking\n",
    "        final_ensemble_proba = y_valid_proba_stack\n",
    "else:\n",
    "    print(f\"  â†’ å–®ä¸€æ¨¡å‹è¼ƒå¥½\")\n",
    "\n",
    "# è¦–è¦ºåŒ– Ensemble æ¯”è¼ƒ\n",
    "fig, axes = plt.subplots(1, 2, figsize=(16, 6))\n",
    "\n",
    "# F1-Score æ¯”è¼ƒ\n",
    "methods = ensemble_comparison['Method'].tolist()\n",
    "f1_scores = ensemble_comparison['F1-Score'].tolist()\n",
    "colors_ensemble = ['#e74c3c', '#2ecc71', '#3498db', '#f39c12', '#9b59b6']\n",
    "\n",
    "axes[0].bar(range(len(methods)), f1_scores, color=colors_ensemble)\n",
    "axes[0].set_xticks(range(len(methods)))\n",
    "axes[0].set_xticklabels(methods, rotation=15, ha='right')\n",
    "axes[0].set_title('F1-Score Comparison: Single Models vs Ensemble', fontsize=14, fontweight='bold')\n",
    "axes[0].set_ylabel('F1-Score', fontsize=12)\n",
    "axes[0].set_ylim([min(f1_scores) - 0.01, max(f1_scores) + 0.01])\n",
    "axes[0].grid(alpha=0.3, axis='y')\n",
    "axes[0].axhline(y=max(f1_scores), color='red', linestyle='--', alpha=0.5, label='Best Performance')\n",
    "for i, v in enumerate(f1_scores):\n",
    "    axes[0].text(i, v + 0.001, f'{v:.4f}', ha='center', fontweight='bold', fontsize=9)\n",
    "axes[0].legend()\n",
    "\n",
    "# ROC-AUC æ¯”è¼ƒ\n",
    "roc_scores = ensemble_comparison['ROC-AUC'].tolist()\n",
    "\n",
    "axes[1].bar(range(len(methods)), roc_scores, color=colors_ensemble)\n",
    "axes[1].set_xticks(range(len(methods)))\n",
    "axes[1].set_xticklabels(methods, rotation=15, ha='right')\n",
    "axes[1].set_title('ROC-AUC Comparison: Single Models vs Ensemble', fontsize=14, fontweight='bold')\n",
    "axes[1].set_ylabel('ROC-AUC', fontsize=12)\n",
    "axes[1].set_ylim([min(roc_scores) - 0.005, max(roc_scores) + 0.005])\n",
    "axes[1].grid(alpha=0.3, axis='y')\n",
    "axes[1].axhline(y=max(roc_scores), color='red', linestyle='--', alpha=0.5, label='Best Performance')\n",
    "for i, v in enumerate(roc_scores):\n",
    "    axes[1].text(i, v + 0.0005, f'{v:.4f}', ha='center', fontweight='bold', fontsize=9)\n",
    "axes[1].legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(os.path.join(OUTPUT_DIR, '03_ensemble_comparison.png'), dpi=300, bbox_inches='tight')\n",
    "plt.close()\n",
    "print(f\"\\nâœ“ Ensemble æ¯”è¼ƒåœ–å·²ä¿å­˜: {OUTPUT_DIR}/03_ensemble_comparison.png\")\n",
    "\n",
    "# =========================\n",
    "# Step 6: åœ¨é©—è­‰é›†ä¸Šè©•ä¼°æœ€ä½³æ¨¡å‹ï¼ˆå¯èƒ½æ˜¯ Ensembleï¼‰\n",
    "# =========================\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"Step 6: åœ¨é©—è­‰é›†ä¸Šè©•ä¼°æœ€ä½³æ¨¡å‹\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "print(f\"\\næœ€ä½³æ¨¡å‹: {best_model_name}\")\n",
    "\n",
    "# æ ¹æ“šæœ€ä½³æ¨¡å‹é¡å‹é€²è¡Œé æ¸¬\n",
    "if 'Ensemble' in best_model_name:\n",
    "    # ä½¿ç”¨ Ensemble é æ¸¬\n",
    "    y_valid_proba = final_ensemble_proba\n",
    "    y_valid_pred = (y_valid_proba >= 0.5).astype(int)\n",
    "else:\n",
    "    # ä½¿ç”¨å–®ä¸€æ¨¡å‹é æ¸¬\n",
    "    y_valid_pred = final_model.predict(X_valid)\n",
    "    y_valid_proba = final_model.predict_proba(X_valid)[:, 1]\n",
    "\n",
    "# è©•ä¼°æŒ‡æ¨™\n",
    "cm_valid = confusion_matrix(y_valid, y_valid_pred)\n",
    "print(\"\\né©—è­‰é›†æ··æ·†çŸ©é™£:\")\n",
    "print(cm_valid)\n",
    "\n",
    "print(\"\\nClassification Report (é©—è­‰é›†):\")\n",
    "print(classification_report(y_valid, y_valid_pred, target_names=['Normal', 'Fraud']))\n",
    "\n",
    "valid_precision = precision_score(y_valid, y_valid_pred, pos_label=1)\n",
    "valid_recall = recall_score(y_valid, y_valid_pred, pos_label=1)\n",
    "valid_f1 = f1_score(y_valid, y_valid_pred, pos_label=1)\n",
    "valid_roc_auc = roc_auc_score(y_valid, y_valid_proba)\n",
    "\n",
    "print(f\"\\né©—è­‰é›†è©³ç´°æŒ‡æ¨™ (Class=1):\")\n",
    "print(f\"  Precision: {valid_precision:.4f}\")\n",
    "print(f\"  Recall: {valid_recall:.4f}\")\n",
    "print(f\"  F1-Score: {valid_f1:.4f}\")\n",
    "print(f\"  ROC-AUC: {valid_roc_auc:.4f}\")\n",
    "\n",
    "# =========================\n",
    "# Step 7: é–¾å€¼èª¿æ•´ (åœ¨é©—è­‰é›†ä¸Šæ±ºå®š) â­\n",
    "# =========================\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"Step 7: é–¾å€¼èª¿æ•´ (åš´è¬¹ç‰ˆæœ¬ - åœ¨é©—è­‰é›†ä¸Šæ±ºå®šæœ€ä½³é–¾å€¼)\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "print(\"\\nâš ï¸ é‡è¦: åœ¨é©—è­‰é›†ä¸Šæ‰¾æœ€ä½³é–¾å€¼ï¼Œæ¸¬è©¦é›†åªç”¨ä¾†æœ€çµ‚è©•ä¼°\")\n",
    "\n",
    "# 7-1. åœ¨é©—è­‰é›†ä¸Šæƒæé–¾å€¼\n",
    "print(\"\\nåœ¨é©—è­‰é›†ä¸Šæƒæé–¾å€¼...\")\n",
    "thresholds = np.arange(0.1, 0.9, 0.05)\n",
    "f1_scores_threshold = []\n",
    "precision_scores = []\n",
    "recall_scores = []\n",
    "\n",
    "for threshold in thresholds:\n",
    "    y_pred_threshold = (y_valid_proba >= threshold).astype(int)\n",
    "    f1 = f1_score(y_valid, y_pred_threshold, pos_label=1)\n",
    "    precision = precision_score(y_valid, y_pred_threshold, pos_label=1)\n",
    "    recall = recall_score(y_valid, y_pred_threshold, pos_label=1)\n",
    "    \n",
    "    f1_scores_threshold.append(f1)\n",
    "    precision_scores.append(precision)\n",
    "    recall_scores.append(recall)\n",
    "\n",
    "# 7-2. æ‰¾å‡ºæœ€ä½³é–¾å€¼\n",
    "best_threshold_idx = np.argmax(f1_scores_threshold)\n",
    "best_threshold = thresholds[best_threshold_idx]\n",
    "best_valid_f1 = f1_scores_threshold[best_threshold_idx]\n",
    "best_valid_precision = precision_scores[best_threshold_idx]\n",
    "best_valid_recall = recall_scores[best_threshold_idx]\n",
    "\n",
    "print(f\"\\nâœ“ åœ¨é©—è­‰é›†ä¸Šæ‰¾åˆ°æœ€ä½³é–¾å€¼\")\n",
    "print(f\"\\næœ€ä½³é–¾å€¼: {best_threshold:.2f}\")\n",
    "print(f\"  é©—è­‰é›† Precision: {best_valid_precision:.4f}\")\n",
    "print(f\"  é©—è­‰é›† Recall: {best_valid_recall:.4f}\")\n",
    "print(f\"  é©—è­‰é›† F1-Score: {best_valid_f1:.4f}\")\n",
    "\n",
    "# 7-3. è¦–è¦ºåŒ–é–¾å€¼èª¿æ•´éç¨‹\n",
    "fig, ax = plt.subplots(figsize=(10, 6))\n",
    "ax.plot(thresholds, precision_scores, 'b-', label='Precision', linewidth=2)\n",
    "ax.plot(thresholds, recall_scores, 'g-', label='Recall', linewidth=2)\n",
    "ax.plot(thresholds, f1_scores_threshold, 'r-', label='F1-Score', linewidth=2)\n",
    "ax.axvline(x=best_threshold, color='orange', linestyle='--', linewidth=2, \n",
    "           label=f'Best Threshold = {best_threshold:.2f}')\n",
    "ax.set_xlabel('Threshold', fontsize=12, fontweight='bold')\n",
    "ax.set_ylabel('Score', fontsize=12, fontweight='bold')\n",
    "ax.set_title('Threshold Tuning on Validation Set', fontsize=14, fontweight='bold')\n",
    "ax.legend()\n",
    "ax.grid(alpha=0.3)\n",
    "plt.savefig(os.path.join(OUTPUT_DIR, '05_threshold_tuning_validation.png'), dpi=300, bbox_inches='tight')\n",
    "plt.close()\n",
    "\n",
    "print(f\"âœ“ é–¾å€¼èª¿æ•´åœ–è¡¨å·²ä¿å­˜: {OUTPUT_DIR}/05_threshold_tuning_validation.png\")\n",
    "\n",
    "# =========================\n",
    "# Step 8: æœ€çµ‚åœ¨æ¸¬è©¦é›†ä¸Šè©•ä¼°ï¼ˆä½¿ç”¨å›ºå®šé–¾å€¼ï¼‰â­\n",
    "# =========================\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"Step 8: æœ€çµ‚æ¸¬è©¦é›†è©•ä¼° (ä½¿ç”¨å›ºå®šé–¾å€¼)\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "print(f\"\\nä½¿ç”¨åœ¨é©—è­‰é›†ä¸Šæ±ºå®šçš„æœ€ä½³é–¾å€¼: {best_threshold:.2f}\")\n",
    "print(\"âš ï¸ æ¸¬è©¦é›†åªè©•ä¼°ä¸€æ¬¡ï¼Œä¸å†èª¿æ•´ä»»ä½•åƒæ•¸\")\n",
    "\n",
    "# 8-1. åœ¨æ¸¬è©¦é›†ä¸Šé æ¸¬\n",
    "if 'Ensemble' in best_model_name:\n",
    "    # ä½¿ç”¨ Ensemble é æ¸¬\n",
    "    # å…ˆç²å–åŸºç¤æ¨¡å‹çš„é æ¸¬\n",
    "    y_test_proba_lgb_base = ensemble_lgb.predict_proba(X_test)[:, 1]\n",
    "    y_test_proba_xgb_base = ensemble_xgb.predict_proba(X_test)[:, 1]\n",
    "    \n",
    "    if best_ensemble_method == 'Simple Average':\n",
    "        y_test_proba = (y_test_proba_lgb_base + y_test_proba_xgb_base) / 2\n",
    "    elif best_ensemble_method == 'Weighted Average':\n",
    "        y_test_proba = (lgb_weight * y_test_proba_lgb_base + \n",
    "                       xgb_weight * y_test_proba_xgb_base)\n",
    "    else:  # Stacking\n",
    "        X_test_stack = np.column_stack([y_test_proba_lgb_base, y_test_proba_xgb_base])\n",
    "        y_test_proba = meta_learner.predict_proba(X_test_stack)[:, 1]\n",
    "else:\n",
    "    # ä½¿ç”¨å–®ä¸€æ¨¡å‹é æ¸¬\n",
    "    y_test_proba = final_model.predict_proba(X_test)[:, 1]\n",
    "\n",
    "y_test_pred = (y_test_proba >= best_threshold).astype(int)  # â­ ä½¿ç”¨å›ºå®šé–¾å€¼\n",
    "\n",
    "# 8-2. è©•ä¼°æŒ‡æ¨™\n",
    "test_precision = precision_score(y_test, y_test_pred, pos_label=1)\n",
    "test_recall = recall_score(y_test, y_test_pred, pos_label=1)\n",
    "test_f1 = f1_score(y_test, y_test_pred, pos_label=1)\n",
    "test_roc_auc = roc_auc_score(y_test, y_test_proba)\n",
    "\n",
    "print(\"\\n\" + \"-\"*80)\n",
    "print(\"æ¸¬è©¦é›†æœ€çµ‚çµæœ:\")\n",
    "print(\"-\"*80)\n",
    "\n",
    "# æ··æ·†çŸ©é™£\n",
    "cm_test = confusion_matrix(y_test, y_test_pred)\n",
    "print(\"\\næ¸¬è©¦é›†æ··æ·†çŸ©é™£:\")\n",
    "print(cm_test)\n",
    "\n",
    "# è¦–è¦ºåŒ–æ··æ·†çŸ©é™£\n",
    "fig, ax = plt.subplots(figsize=(8, 6))\n",
    "disp = ConfusionMatrixDisplay(confusion_matrix=cm_test, display_labels=['Normal', 'Fraud'])\n",
    "disp.plot(ax=ax, cmap='Blues', values_format='d')\n",
    "plt.title(f'{best_model_name} - Confusion Matrix (Test Set)', fontsize=14, fontweight='bold')\n",
    "plt.savefig(os.path.join(OUTPUT_DIR, '06_confusion_matrix_test.png'), dpi=300, bbox_inches='tight')\n",
    "plt.close()\n",
    "print(f\"âœ“ æ··æ·†çŸ©é™£å·²ä¿å­˜: {OUTPUT_DIR}/06_confusion_matrix_test.png\")\n",
    "\n",
    "# Classification Report\n",
    "print(\"\\nClassification Report (æ¸¬è©¦é›†):\")\n",
    "print(classification_report(y_test, y_test_pred, target_names=['Normal', 'Fraud']))\n",
    "\n",
    "print(f\"\\næ¸¬è©¦é›†è©³ç´°æŒ‡æ¨™ (Class=1):\")\n",
    "print(f\"  Precision: {test_precision:.4f}\")\n",
    "print(f\"  Recall: {test_recall:.4f}\")\n",
    "print(f\"  F1-Score: {test_f1:.4f}\")\n",
    "print(f\"  ROC-AUC: {test_roc_auc:.4f}\")\n",
    "\n",
    "# =========================\n",
    "# Step 9: æ¨¡å‹è§£é‡‹ (SHAP & LIME)\n",
    "# =========================\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"Step 9: æ¨¡å‹è§£é‡‹ (SHAP & LIME)\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# 9-1. SHAP è§£é‡‹\n",
    "print(\"\\nä½¿ç”¨ SHAP é€²è¡Œæ¨¡å‹è§£é‡‹...\")\n",
    "\n",
    "# ä½¿ç”¨æ¸¬è©¦é›†çš„ä¸€éƒ¨åˆ†ï¼ˆ200 ç­†ï¼‰\n",
    "X_test_sample = X_test.iloc[:200]\n",
    "print(f\"  ä½¿ç”¨ {len(X_test_sample)} ç­†æ¸¬è©¦è³‡æ–™é€²è¡Œ SHAP åˆ†æ\")\n",
    "\n",
    "# é¸æ“‡ç”¨æ–¼ SHAP è§£é‡‹çš„æ¨¡å‹\n",
    "if 'Ensemble' in best_model_name:\n",
    "    print(f\"  âš ï¸ æœ€ä½³æ¨¡å‹æ˜¯ Ensembleï¼Œä½¿ç”¨ {lgb_ensemble_name} é€²è¡Œ SHAP åˆ†æ\")\n",
    "    shap_model = ensemble_lgb\n",
    "    shap_model_name = lgb_ensemble_name\n",
    "else:\n",
    "    shap_model = final_model\n",
    "    shap_model_name = best_model_name\n",
    "\n",
    "# å»ºç«‹ SHAP explainer\n",
    "if 'LightGBM' in shap_model_name or 'XGBoost' in shap_model_name:\n",
    "    explainer = shap.TreeExplainer(shap_model)\n",
    "    shap_values = explainer.shap_values(X_test_sample)\n",
    "    \n",
    "    if isinstance(shap_values, list):\n",
    "        shap_values = shap_values[1]\n",
    "\n",
    "print(\"  âœ“ SHAP å€¼è¨ˆç®—å®Œæˆ\")\n",
    "\n",
    "# SHAP Summary Plot (æ¢ç‹€åœ–)\n",
    "print(\"\\n  ç¹ªè£½ SHAP ç‰¹å¾µé‡è¦æ€§åœ–...\")\n",
    "plt.figure(figsize=(10, 8))\n",
    "shap.summary_plot(shap_values, X_test_sample, plot_type=\"bar\", show=False, max_display=20)\n",
    "plt.title('SHAP Feature Importance (Top 20)', fontsize=14, fontweight='bold')\n",
    "plt.tight_layout()\n",
    "plt.savefig(os.path.join(OUTPUT_DIR, '07_shap_importance.png'), dpi=300, bbox_inches='tight')\n",
    "plt.close()\n",
    "print(f\"  âœ“ SHAP ç‰¹å¾µé‡è¦æ€§åœ–å·²ä¿å­˜: {OUTPUT_DIR}/07_shap_importance.png\")\n",
    "\n",
    "# SHAP Summary Plot (é»åœ–)\n",
    "print(\"\\n  ç¹ªè£½ SHAP æ‘˜è¦åœ–...\")\n",
    "plt.figure(figsize=(10, 8))\n",
    "shap.summary_plot(shap_values, X_test_sample, show=False, max_display=20)\n",
    "plt.title('SHAP Feature Impact Summary (Top 20)', fontsize=14, fontweight='bold')\n",
    "plt.tight_layout()\n",
    "plt.savefig(os.path.join(OUTPUT_DIR, '08_shap_summary.png'), dpi=300, bbox_inches='tight')\n",
    "plt.close()\n",
    "print(f\"  âœ“ SHAP æ‘˜è¦åœ–å·²ä¿å­˜: {OUTPUT_DIR}/08_shap_summary.png\")\n",
    "\n",
    "# 9-2. LIME è§£é‡‹\n",
    "print(\"\\nä½¿ç”¨ LIME é€²è¡Œå€‹åˆ¥äº¤æ˜“è§£é‡‹...\")\n",
    "\n",
    "# å»ºç«‹ LIME explainer\n",
    "# æ ¹æ“šæ¨¡å‹é¡å‹é¸æ“‡è¨“ç·´è³‡æ–™\n",
    "if 'Ensemble' in best_model_name:\n",
    "    if 'class_weight' in lgb_ensemble_name or 'scale_pos_weight' in lgb_ensemble_name:\n",
    "        lime_train_data = X_train.values\n",
    "    else:\n",
    "        lime_train_data = X_train_smote.values\n",
    "    lime_feature_names = X_train.columns.tolist()\n",
    "else:\n",
    "    if best_model_name in ['LightGBM (class_weight)', 'XGBoost (scale_pos_weight)']:\n",
    "        lime_train_data = X_train.values\n",
    "        lime_feature_names = X_train.columns.tolist()\n",
    "    else:\n",
    "        lime_train_data = X_train_smote.values\n",
    "        lime_feature_names = X_train_smote.columns.tolist()\n",
    "\n",
    "lime_explainer = lime_tabular.LimeTabularExplainer(\n",
    "    lime_train_data,\n",
    "    feature_names=lime_feature_names,\n",
    "    class_names=['Normal', 'Fraud'],\n",
    "    mode='classification',\n",
    "    random_state=RANDOM_STATE\n",
    ")\n",
    "\n",
    "# é¸æ“‡è©æ¬ºå’Œæ­£å¸¸äº¤æ˜“å„ä¸€ç­†\n",
    "fraud_idx = y_test[y_test == 1].index[0]\n",
    "normal_idx = y_test[y_test == 0].index[0]\n",
    "\n",
    "# å®šç¾©é æ¸¬å‡½æ•¸ï¼ˆæ”¯æŒ Ensembleï¼‰\n",
    "def lime_predict_fn(X_data):\n",
    "    if 'Ensemble' in best_model_name:\n",
    "        # Ensemble é æ¸¬\n",
    "        proba_lgb = ensemble_lgb.predict_proba(X_data)\n",
    "        proba_xgb = ensemble_xgb.predict_proba(X_data)\n",
    "        \n",
    "        if best_ensemble_method == 'Simple Average':\n",
    "            return (proba_lgb + proba_xgb) / 2\n",
    "        elif best_ensemble_method == 'Weighted Average':\n",
    "            return lgb_weight * proba_lgb + xgb_weight * proba_xgb\n",
    "        else:  # Stacking\n",
    "            X_stack = np.column_stack([proba_lgb[:, 1], proba_xgb[:, 1]])\n",
    "            return meta_learner.predict_proba(X_stack)\n",
    "    else:\n",
    "        # å–®ä¸€æ¨¡å‹é æ¸¬\n",
    "        return final_model.predict_proba(X_data)\n",
    "\n",
    "print(f\"\\n  è§£é‡‹è©æ¬ºäº¤æ˜“ (ç´¢å¼•: {fraud_idx})...\")\n",
    "exp_fraud = lime_explainer.explain_instance(\n",
    "    X_test.loc[fraud_idx].values,\n",
    "    lime_predict_fn,\n",
    "    num_features=10\n",
    ")\n",
    "\n",
    "print(f\"  è§£é‡‹æ­£å¸¸äº¤æ˜“ (ç´¢å¼•: {normal_idx})...\")\n",
    "exp_normal = lime_explainer.explain_instance(\n",
    "    X_test.loc[normal_idx].values,\n",
    "    lime_predict_fn,\n",
    "    num_features=10\n",
    ")\n",
    "\n",
    "# å„²å­˜ LIME è§£é‡‹\n",
    "exp_fraud.save_to_file(os.path.join(OUTPUT_DIR, '09_lime_fraud.html'))\n",
    "exp_normal.save_to_file(os.path.join(OUTPUT_DIR, '10_lime_normal.html'))\n",
    "\n",
    "print(f\"  âœ“ LIME è§£é‡‹å·²ä¿å­˜:\")\n",
    "print(f\"    - è©æ¬ºäº¤æ˜“: {OUTPUT_DIR}/09_lime_fraud.html\")\n",
    "print(f\"    - æ­£å¸¸äº¤æ˜“: {OUTPUT_DIR}/10_lime_normal.html\")\n",
    "\n",
    "# 9-3. ç‰¹å¾µé‡è¦æ€§åˆ†æ\n",
    "print(\"\\n\" + \"-\"*80)\n",
    "print(\"æ¨¡å‹è§£é‡‹åˆ†æ:\")\n",
    "print(\"-\"*80)\n",
    "\n",
    "shap_importance = np.abs(shap_values).mean(axis=0)\n",
    "top_features_idx = np.argsort(shap_importance)[::-1][:10]\n",
    "top_features = X_test_sample.columns[top_features_idx]\n",
    "\n",
    "print(\"\\næœ€é‡è¦çš„ 10 å€‹ç‰¹å¾µ (ä¾ç…§ SHAP é‡è¦æ€§):\")\n",
    "for i, feat in enumerate(top_features, 1):\n",
    "    print(f\"  {i}. {feat}\")\n",
    "\n",
    "# =========================\n",
    "# Step 10: æ•´é«”ç¸½çµèˆ‡è¼¸å‡º\n",
    "# =========================\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"Step 10: æ•´é«”ç¸½çµèˆ‡è¼¸å‡º\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# 10-1. ç¸½çµå ±å‘Š\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"ğŸ¯ æœ€çµ‚æ¨¡å‹ç¸½çµå ±å‘Š (åš´è¬¹ç‰ˆæœ¬)\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "print(f\"\\næœ€ä½³æ¨¡å‹: {best_model_name}\")\n",
    "print(f\"ä¸å¹³è¡¡è™•ç†æ–¹å¼: SMOTE\")\n",
    "print(f\"é–¾å€¼: {best_threshold:.2f} (åœ¨é©—è­‰é›†ä¸Šæ±ºå®š)\")\n",
    "\n",
    "print(f\"\\né©—è­‰é›†æ€§èƒ½ (é–¾å€¼={best_threshold:.2f}):\")\n",
    "print(f\"  - Precision: {best_valid_precision:.4f}\")\n",
    "print(f\"  - Recall: {best_valid_recall:.4f}\")\n",
    "print(f\"  - F1-Score: {best_valid_f1:.4f}\")\n",
    "\n",
    "print(f\"\\næ¸¬è©¦é›†æ€§èƒ½ (ä½¿ç”¨å›ºå®šé–¾å€¼={best_threshold:.2f}):\")\n",
    "print(f\"  - Precision: {test_precision:.4f}\")\n",
    "print(f\"  - Recall: {test_recall:.4f}\")\n",
    "print(f\"  - F1-Score: {test_f1:.4f}\")\n",
    "print(f\"  - ROC-AUC: {test_roc_auc:.4f}\")\n",
    "\n",
    "print(\"\\nåš´è¬¹æ€§æ”¹é€²:\")\n",
    "print(\"  âœ… V1~V28 æ˜¯ PCA ç‰¹å¾µï¼Œå·²æ¨™æº–åŒ–ï¼Œä¸éœ€é¡å¤– scaling\")\n",
    "print(\"  âœ… åŠ å…¥ Logistic Regression baseline\")\n",
    "print(\"  âœ… é–¾å€¼åœ¨é©—è­‰é›†ä¸Šæ±ºå®šï¼Œæ¸¬è©¦é›†åªè©•ä¼°ä¸€æ¬¡\")\n",
    "\n",
    "print(\"\\né—œéµç™¼ç¾:\")\n",
    "print(f\"  1. æ¨¡å‹æˆåŠŸæ•æ‰åˆ° {test_recall*100:.2f}% çš„è©æ¬ºäº¤æ˜“\")\n",
    "print(f\"  2. é æ¸¬ç‚ºè©æ¬ºçš„äº¤æ˜“ä¸­ï¼Œæœ‰ {test_precision*100:.2f}% æ˜¯çœŸçš„è©æ¬º\")\n",
    "print(f\"  3. æ¸¬è©¦é›† F1-Score: {test_f1:.4f}\")\n",
    "\n",
    "# 10-2. ä¿å­˜é æ¸¬çµæœ\n",
    "print(\"\\n\" + \"-\"*80)\n",
    "print(\"ä¿å­˜é æ¸¬çµæœ...\")\n",
    "print(\"-\"*80)\n",
    "\n",
    "# å»ºç«‹çµæœ DataFrame\n",
    "results_df = X_test.copy()\n",
    "results_df['Actual_Class'] = y_test\n",
    "results_df['Predicted_Class'] = y_test_pred\n",
    "results_df['Fraud_Probability'] = y_test_proba\n",
    "results_df['Correct_Prediction'] = (results_df['Actual_Class'] == results_df['Predicted_Class'])\n",
    "\n",
    "# ä¿å­˜åˆ° CSV\n",
    "results_df.to_csv(os.path.join(OUTPUT_DIR, 'prediction_results.csv'), index=False)\n",
    "print(f\"âœ“ é æ¸¬çµæœå·²ä¿å­˜: {OUTPUT_DIR}/prediction_results.csv\")\n",
    "\n",
    "print(f\"\\næª”æ¡ˆåŒ…å« {len(results_df)} ç­†æ¸¬è©¦è³‡æ–™çš„é æ¸¬çµæœ\")\n",
    "\n",
    "# çµ±è¨ˆé æ¸¬æ­£ç¢ºç‡\n",
    "accuracy = results_df['Correct_Prediction'].mean()\n",
    "print(f\"æ•´é«”æº–ç¢ºç‡: {accuracy*100:.2f}%\")\n",
    "\n",
    "# 10-3. ç”Ÿæˆæª”æ¡ˆæ¸…å–®\n",
    "print(\"\\n\" + \"-\"*80)\n",
    "print(\"ç”Ÿæˆçš„æª”æ¡ˆæ¸…å–®:\")\n",
    "print(\"-\"*80)\n",
    "\n",
    "files = [\n",
    "    '01_time_distribution.png',\n",
    "    '02_amount_distribution.png',\n",
    "    '03_model_comparison_all.png',\n",
    "    '03_ensemble_comparison.png',\n",
    "    '04_confusion_matrices_comparison.png',\n",
    "    '05_threshold_tuning_validation.png',\n",
    "    '06_confusion_matrix_test.png',\n",
    "    '07_shap_importance.png',\n",
    "    '08_shap_summary.png',\n",
    "    '09_lime_fraud.html',\n",
    "    '10_lime_normal.html',\n",
    "    'prediction_results.csv'\n",
    "]\n",
    "\n",
    "for i, file in enumerate(files, 1):\n",
    "    print(f\"  {i}. {OUTPUT_DIR}/{file}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"âœ… æ‰€æœ‰æ­¥é©Ÿå®Œæˆï¼(åš´è¬¹ç‰ˆæœ¬)\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "print(f\"\\nåŸ·è¡Œæ™‚é–“: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\")\n",
    "print(\"\\næ„Ÿè¬ä½¿ç”¨ï¼é€™å€‹ç‰ˆæœ¬æ›´ç¬¦åˆæ©Ÿå™¨å­¸ç¿’æœ€ä½³å¯¦è¸ã€‚\")\n",
    "print(\"=\"*80)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6066b76",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (XGBoost)",
   "language": "python",
   "name": "xgboost-env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
